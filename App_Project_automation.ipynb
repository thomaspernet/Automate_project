{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# App "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import display\n",
    "import re, json, datetime, argparse, sys, os, ast, shutil\n",
    "import nbformat as nbf\n",
    "from Fast_connectCloud import connector\n",
    "from GoogleDrivePy.google_drive import connect_drive\n",
    "from GoogleDrivePy.google_platform import connect_cloud_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+git://github.com/thomaspernet/GoogleDrive-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "# Create Project and Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "name_project = widgets.Text(\n",
    "       value=\"Find an Awesome name\",\n",
    "       description='Project name', layout=Layout(width='50%', height='80px') )\n",
    "\n",
    "path_src = widgets.Text(\n",
    "       value=\"/Users/thomas/Google Drive/Projects/Data_science/GitHub/\" \\\n",
    "\"Template_project_Github\",\n",
    "       description='path Projects', layout=Layout(width='50%', height='80px') )\n",
    "path_dest = widgets.Text(\n",
    "       value='/Users/thomas/Google Drive/Projects/Data_science/GitHub/Repositories',\n",
    "       description='Destination Projects', layout=Layout(width='50%', height='80px') )\n",
    "checkbox = widgets.Checkbox(\n",
    "           description='Create Github repo',value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "butt_git = widgets.Button(description='Add Project')\n",
    "outt_git = widgets.Output()\n",
    "\n",
    "def on_butt_clicked_git(_):\n",
    "    print('Begin Create project')\n",
    "    new_path = os.path.join(path_dest.value, name_project.value)\n",
    "    os.mkdir(path=new_path)\n",
    "\n",
    "    def copytree(src, dst, symlinks=False, ignore=None):\n",
    "        for item in os.listdir(src):\n",
    "            s = os.path.join(src, item)\n",
    "            d = os.path.join(dst, item)\n",
    "            if os.path.isdir(s):\n",
    "                shutil.copytree(s, d, symlinks, ignore)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "\n",
    "    copytree(path_src.value,new_path)\n",
    "\n",
    "    ### Rename project\n",
    "    with open(os.path.join(new_path, 'README.md'), \"r\") as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    lines = lines.replace('PROJECTNAME',\n",
    "                          name_project.value)\n",
    "\n",
    "    with open(os.path.join(new_path, 'README.md'), 'w') as file:\n",
    "        file.write(lines)\n",
    "        \n",
    "    print('Project Created here: {}'.format(path_dest.value))\n",
    "    \n",
    "    if checkbox.value:\n",
    "        os.chdir(new_path)\n",
    "\n",
    "    ### Config the hub file\n",
    "    ### https://hub.github.com/ -> for mac\n",
    "    ####https://github.com/github/hub/issues/1067#issuecomment-482133220\n",
    "\n",
    "        os.system(\"git init\")\n",
    "        os.system(\"git add .\")\n",
    "        os.system('git commit -m \"Create proejct\"')\n",
    "    ### Create repo github\n",
    "        os.system('hub create')\n",
    "    ### Oush to GitHub\n",
    "        push =\"git remote add origin https://github.com/thomaspernet/{}\".format(\n",
    "        name_project.value)\n",
    "        os.system(push)\n",
    "        os.system(\"git push -u origin master\")\n",
    "        print('Project {0} added to github. Please open at https://github.com/thomaspernet/{0}'.format(\n",
    "            name_project.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Create project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bef9bb05a54eb784a590dbad0a71a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Find an Awesome name', description='Project name', layout=Layout(height='80px', widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Create project\n",
      "Project Created here: /Users/thomas/Google Drive/Projects/Data_science/GitHub/Repositories\n",
      "Project Coda_test added to github. Please open at https://github.com/thomaspernet/Coda_test\n"
     ]
    }
   ],
   "source": [
    "butt_git.on_click(on_butt_clicked_git)\n",
    "box = widgets.VBox([name_project,\n",
    "                    path_src,\n",
    "                    path_dest,\n",
    "                    checkbox,\n",
    "                    butt_git,outt_git])\n",
    "box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Create Notebook Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective\n",
    "menu = widgets.Dropdown(\n",
    "       options=['Connect_source', 'Analytics', 'Studio'],\n",
    "       value='Connect_source',\n",
    "       description='Select:')\n",
    "menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective\n",
    "connector_engine = widgets.Dropdown(\n",
    "       options=['GS', 'BG'],\n",
    "       value='GS',\n",
    "       description='Select:')\n",
    "connector_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "labels = widgets.Text(\n",
    "       description='Variables label', \n",
    "    layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date var -> srtudio\n",
    "date_var = widgets.Text(\n",
    "       description='Date Variable (studio)', \n",
    "    layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some widgets\n",
    "text = widgets.Text(\n",
    "       value='My First Notebook',\n",
    "       description='Notebook Name', layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = widgets.Text(\n",
    "       value='/Users/thomas/Google Drive/Projects/Data_science/GitHub/Repositories',\n",
    "       description='path Projects', layout=Layout(width='50%', height='80px') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top = os.listdir(path.value)\n",
    "list_project = []\n",
    "for name in top:\n",
    "    list_project.append(name)\n",
    "output_prject = widgets.Output()\n",
    "dropdown_prject = widgets.Dropdown(options =list_project,\n",
    "                                  description='Project name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = widgets.Text(\n",
    "       value='/Users/Thomas/Google Drive/Projects/Data_science/Google_code_n_Oauth/Client_Oauth/Google_auth/',\n",
    "       description='path creds Google', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = connector.open_connection(online_connection=False,\n",
    "                               path_credential=creds.value)\n",
    "\n",
    "service_gd = gs.connect_remote(engine='GS')\n",
    "service_gcp = gs.connect_remote(engine='GCP')\n",
    "\n",
    "gdr = connect_drive.connect_drive(service_gd['GoogleDrive'])\n",
    "\n",
    "gcp = connect_cloud_platform.connect_console(project='valid-pagoda-132423',\n",
    "                                             service_account=service_gcp['GoogleCloudP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dropdown spreadsheet\n",
    "sheetid = '1fB-h1Sbdy7wkUghQTugYKBbvNS5plFCc3TCK8zWe71M'\n",
    "sheetname = 'data'\n",
    "\n",
    "spreadsheets = gdr.upload_data_from_spreadsheet(sheetID = sheetid,\n",
    "sheetName = sheetname,to_dataframe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### recherche spreadsheet\n",
    "def findspreadsheet(spreadsheet_name):\n",
    "    sheetID = gdr.find_file_id(file_name = spreadsheet_name,\n",
    "                           to_print = False)\n",
    "    \n",
    "    list_ = gdr.listSpreadsheet(sheetID= sheetID)\n",
    "    \n",
    "    return list_\n",
    "\n",
    "#### Spreadsheet\n",
    "def return_spread(theme, module#, fournisseur\n",
    "             ):\n",
    "    dic_ = {\n",
    "        'spreadsheet':theme,\n",
    "        'sheet':module\n",
    "    }\n",
    "    return dic_\n",
    "\n",
    "# Spreadsheet\n",
    "unique_spreadsheet = (spreadsheets.loc[lambda x: \n",
    "                                 x['Storage'].isin(['GS'])]['Filename']\n",
    "                .sort_values().to_list())\n",
    "spreadW = widgets.Dropdown(options=unique_spreadsheet)\n",
    "init = spreadW.value\n",
    "\n",
    "### Sheet\n",
    "sheetW = widgets.Dropdown(options=\n",
    "                           findspreadsheet(init),\n",
    "                          description = 'Select Sheet'\n",
    "                          )\n",
    "def select_sheet(theme):\n",
    "    sheetW.options = findspreadsheet(theme)\n",
    "\n",
    "\n",
    "i = widgets.interactive(select_sheet, theme=spreadW)\n",
    "\n",
    "#j = widgets.interactive(\n",
    "#    return_spread,\n",
    "#    theme=spreadW,\n",
    "#    module=sheetW,\n",
    "    #fournisseur=fournisseurtW\n",
    "#)\n",
    "butt = widgets.Button(description='Add Spreadsheets')\n",
    "outt = widgets.Output()\n",
    "input_datasets = []\n",
    "sheetnames = []\n",
    "def on_butt_clicked(_):\n",
    "    #outt.clear_output()\n",
    "    with outt:\n",
    "        dic_ = return_spread(spreadW.value, sheetW.value)\n",
    "        input_datasets.append(dic_['spreadsheet'])\n",
    "        sheetnames.append(dic_['sheet'])\n",
    "        \n",
    "        display(input_datasets, sheetnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spread_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list Spreadsheet', layout=Layout(width='50%', height='80px'))\n",
    "sheet_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list sheet', layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### recherche tabel\n",
    "def findtable(dataset):\n",
    "    \n",
    "    list_ = gcp.list_tables(dataset = dataset)['tables']\n",
    "    \n",
    "    return list_\n",
    "\n",
    "#### table\n",
    "def return_table(theme, module#, fournisseur\n",
    "             ):\n",
    "    dic_ = {\n",
    "        'dataset':theme,\n",
    "        'table':module\n",
    "    }\n",
    "    return dic_\n",
    "\n",
    "# Theme\n",
    "unique_dataset = gcp.list_dataset()['Dataset']\n",
    "datasetsdW = widgets.Dropdown(options=unique_dataset)\n",
    "init = datasetsdW.value\n",
    "\n",
    "### Theme: Module\n",
    "\n",
    "tablesW = widgets.Dropdown(options=\n",
    "                           findtable(init),\n",
    "                           description = 'Select Table'\n",
    "                          )\n",
    "init2 = tablesW.value\n",
    "\n",
    "\n",
    "def select_table(theme):\n",
    "    tablesW.options = findtable(theme)\n",
    "\n",
    "i = widgets.interactive(select_table, theme=datasetsdW)\n",
    "\n",
    "#j = widgets.interactive(\n",
    "#    return_spread,\n",
    "#    theme=spreadW,\n",
    "#    module=sheetW,\n",
    "    #fournisseur=fournisseurtW\n",
    "#)\n",
    "butt1 = widgets.Button(description='Add table')\n",
    "outt1 = widgets.Output()\n",
    "input_dataset = []\n",
    "tables = []\n",
    "\n",
    "def on_butt_clicked1(_):\n",
    "    #outt.clear_output()\n",
    "    with outt1:\n",
    "        dic_ = return_table(datasetsdW.value, tablesW.value)\n",
    "        input_dataset.append(dic_['dataset'])\n",
    "        tables.append(dic_['table'])\n",
    "        \n",
    "        display(input_dataset, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list Dataset', layout=Layout(width='50%', height='80px'))\n",
    "table_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list Table', layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_notebook_processing(params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    notebook_name = params['notebook_name']\n",
    "    project_name = params['project_name']\n",
    "    input_datasets = params['input_datasets']\n",
    "    sheetnames = params['sheetnames']\n",
    "    bigquery_dataset = params['bigquery_dataset']\n",
    "    destination_engine = params['destination_engine']\n",
    "    path_ = params['path_processing']\n",
    "    path_notebook = params['path_notebook_processing']\n",
    "    project='valid-pagoda-132423'\n",
    "    username = \"thomas\"\n",
    "    \n",
    "    with open(path_notebook) as f:\n",
    "        ipynb = json.load(f)\n",
    "\n",
    "    nb_cells = []\n",
    "    output_str = \"\"\n",
    "\n",
    "# Parse cells in Jupyter notebook template\n",
    "    for cell in ipynb['cells']:\n",
    "        cell_type = cell['cell_type']\n",
    "        source = cell['source']\n",
    "        nb_cells.append([cell_type, source])\n",
    "\n",
    "## Change header\n",
    "    nb_cells[0][1][0] = nb_cells[0][1][0].replace('TITLE', notebook_name)\n",
    "    now = datetime.datetime.now()\n",
    "    nb_cells[0][1][2] = nb_cells[0][1][2].replace('XX',\n",
    "     now.strftime(\"%Y-%m-%d %H:%M %Z\"))\n",
    "\n",
    "### Add Project name \n",
    "\n",
    "    nb_cells[5][1][8] = nb_cells[5][1][8].replace('PROJECTNAME', project_name) ### Pagoda\n",
    "\n",
    "##### Find if prefix in list of dtaset\n",
    "###### If dataset found, then remove from list, and make\n",
    "##### special variable for special treatment\n",
    "##### So far only one preffix dataset can be added\n",
    "\n",
    "    regex = r\"\\*\"\n",
    "    for d in input_datasets:\n",
    "        matches = re.search(regex, d)\n",
    "        if matches:\n",
    "            dataset_suffix = d.replace(\"*\", \"\")\n",
    "            input_datasets.remove(d)\n",
    "        else:\n",
    "            try:\n",
    "                if dataset_suffix:\n",
    "                    pass\n",
    "            except:\n",
    "                dataset_suffix = False\n",
    "\n",
    "## Add data source\n",
    "## Load dataset\n",
    "    datasets = gcp.list_dataset()\n",
    "    bq_contents = []\n",
    "### To recover the index of the sheetname in sheetname\n",
    "\n",
    "    for key, value in datasets.items():\n",
    "        for dataset in value:\n",
    "            tables = gcp.list_tables(dataset = dataset)\n",
    "            bq_contents.append(tables)\n",
    "## Load Buckets\n",
    "    buckets = gcp.list_bucket()\n",
    "    gcs_contents = []\n",
    "    for key, value in buckets.items():\n",
    "        for bucket in value:\n",
    "            blob = gcp.list_blob(bucket = bucket)\n",
    "            gcs_contents.append(blob)\n",
    "\n",
    "            dic_result = {\n",
    "                'BQ': {'Filename': []},\n",
    "                'GCS': {'Filename': []},\n",
    "                'GS': {'Filename': [],\n",
    "                       'ID': [],\n",
    "                       'sheetname':[]},\n",
    "            }\n",
    "            sheetindex = 0\n",
    "\n",
    "            for l_dataset in input_datasets:\n",
    "                found = False\n",
    "                ### Search in BQ\n",
    "                for i in range(0, len(bq_contents)):\n",
    "                    regex = r'(\\w*{0}\\w*)'.format(l_dataset)\n",
    "                    matches = re.search(regex, str(bq_contents[i]))\n",
    "                    if matches:\n",
    "\t\t\t\t\t# print(l_dataset)\n",
    "                        dic_result['BQ']['Filename'].append(l_dataset)\n",
    "                        found = True\n",
    "\t\t\t### Search in GCS\n",
    "                if found == False:\n",
    "                    found = False\n",
    "                    for i in range(0, len(gcs_contents)):\n",
    "                        regex = r'(\\w*{0}\\w*)'.format(l_dataset)\n",
    "                        matches = re.search(regex, str(gcs_contents[i]))\n",
    "                        if matches:\n",
    "                            dic_result['GCS']['Filename'].append(l_dataset)\n",
    "                            found = True\n",
    "\n",
    "                if found == False:\n",
    "                    found = False\n",
    "\t\t\t\t\t### Search in GS\n",
    "                    ID_gS = gdr.find_file_id(file_name=l_dataset)\n",
    "\n",
    "                    if sheetindex < len(sheetnames):\n",
    "                        sheetName = sheetnames[sheetindex]\n",
    "                        dic_result['GS']['sheetname'].append(sheetName)\n",
    "                        dic_result['GS']['Filename'].append(l_dataset)\n",
    "                        dic_result['GS']['ID'].append(ID_gS)\n",
    "                        sheetindex += 1\n",
    "\n",
    "### Add destination\n",
    "\n",
    "    if destination_engine == 'GCP':\n",
    "        nb_cells[2][1][5] = nb_cells[2][1][5].replace('None',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  notebook_name + '.gz')\n",
    "        nb_cells[2][1][6] = nb_cells[2][1][6].replace('None',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  notebook_name)\n",
    "    else:\n",
    "        nb_cells[2][1][4] = nb_cells[2][1][4].replace('None',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  notebook_name)\n",
    "\n",
    "### Add load data\n",
    "    dic_load_data = {\n",
    "\t'BQ':{'markdown': [],\n",
    "\t\t\t'code':[]\n",
    "\t\t   },\n",
    "\t'GCS': {'markdown': [],\n",
    "\t\t\t'code':[]\n",
    "\t\t   },\n",
    "\t'GS':{'markdown': [],\n",
    "\t\t\t'code':[]\n",
    "\t\t   }\n",
    "}\n",
    "    template_gcs_load = \"\"\"\n",
    "gcp.download_blob(bucket_name = '{0}',\n",
    "\t\t\t\t  destination_blob_name = '{1}',\n",
    "\t\t\t\t  source_file_name = '{2}')\n",
    "\n",
    "    df_{3} = pd.read_csv('{4}',\n",
    "\t\t\t\t\t  compression='gzip',\n",
    "\t\t\t\t\t  header=0,\n",
    "\t\t\t\t\t  sep=',',\n",
    "\t\t\t\t\t  quotechar='\"',\n",
    "\t\t\t\t\t  error_bad_lines=False\n",
    "\t\t\t\t\t  )\n",
    "    \"\"\"\n",
    "\n",
    "    template_gcs_md = \"\"\"\n",
    "## Load {0} from Google Cloud Storage\n",
    "\n",
    "    Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "#### GCS\n",
    "##### Only one file from  GCS?\n",
    "\n",
    "    if dic_result['GCS']['Filename']:\n",
    "        for f, file in enumerate(dic_result['GCS']['Filename']):\n",
    "            filename = dic_result['GCS']['Filename'][f]\n",
    "            for p, bucket in enumerate(gcs_contents):\n",
    "                for i, blob in enumerate(bucket['blob']):\n",
    "                    regex = r'(\\w*{0}\\w*)'.format(file)\n",
    "                    matches = re.search(regex, str(blob))\n",
    "\t\t\t\t\n",
    "                    if matches:\n",
    "                        df_ = file.split(\".\")[0]\n",
    "\t\t\t\t\t#path = gcs_contents[p]['blob'][3].replace(\n",
    "\t\t\t\t\t#'/' + file, '')\n",
    "\t\t\t\t\t### get path\n",
    "                        regex = r\"/[^/]*$\"\n",
    "                        path = re.sub(regex, \"\", blob) \n",
    "                        bucket_name  = bucket['Bucket']\n",
    "                        ggs_cell = template_gcs_load.format(bucket_name,\n",
    "\t\t\t\t\t\t\t\t\t\t path,\n",
    "\t\t\t\t\t\t\t\t\t\t filename,\n",
    "\t\t\t\t\t\t\t\t\t\t df_,\n",
    "\t\t\t\t\t\t\t\t\t\t filename\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   )\n",
    "                        ggs_md = template_gcs_md.format(filename)\n",
    "                        dic_load_data['GCS']['markdown'].append(ggs_md)\n",
    "                        dic_load_data['GCS']['code'].append(ggs_cell)\n",
    "\n",
    "        dic_destination_inf = {\n",
    "            'Bucket':bucket_name,\n",
    "            'destination_blob_name':path,\n",
    "            'new_destination_blob': path.replace('Raw_', 'Processed_')\n",
    "\t}\n",
    "    else:\n",
    "\t#### We don't know where to save the dataset if no table uploaded from GCS\n",
    "        dic_destination_inf = {\n",
    "            'Bucket':'NEED TO DEFINE',\n",
    "            'destination_blob_name':'NEED TO DEFINE',\n",
    "            'new_destination_blob':  'XXXXX/Processed_'\n",
    "\t}\n",
    "##### GBQ\n",
    "    template_gbq_load = \"\"\"\n",
    "      query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM {0}.{1} \"\n",
    "\n",
    "        )\n",
    "\n",
    "    df_{1} = gcp.upload_data_from_bigquery(query = query, location = 'US')\n",
    "    df_{1}.head()\n",
    "    \"\"\"\n",
    "\n",
    "    template_gbq_md = \"\"\"\n",
    "    ## Load {0} from Google Big Query\n",
    "\n",
    "    Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    \"\"\"\n",
    "    if dic_result['BQ']['Filename']:\n",
    "        for f, file in enumerate(dic_result['BQ']['Filename']):\n",
    "            filename = dic_result['BQ']['Filename'][f]\n",
    "#### Note need to search for thr bigquery dataset. Right now, use the predefined one in the argument\n",
    "            cell_gbq = template_gbq_load.format(bigquery_dataset,\n",
    "                filename)\n",
    "            cell_md = template_gbq_md.format(filename)\n",
    "            dic_load_data['BQ']['markdown'].append(cell_md)\n",
    "            dic_load_data['BQ']['code'].append(cell_gbq)\n",
    "\n",
    "##### GS\n",
    "    template_gs_load = \"\"\"\n",
    "    ### Please go here {0}\n",
    "    ### To change the range\n",
    "\n",
    "    sheetid = '{1}'\n",
    "    sheetname = '{2}'\n",
    "\n",
    "    df_{3} = gdr.upload_data_from_spreadsheet(sheetID = sheetid,\n",
    "    sheetName = sheetname,\n",
    "         to_dataframe = True)\n",
    "    df_{3}.head()\n",
    "    \"\"\"\n",
    "\n",
    "    template_gs_md = \"\"\"\n",
    "    ## Load {0} from Google Spreadsheet\n",
    "\n",
    "    Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    Profiling will be available soon for this dataset\n",
    "\n",
    "    \"\"\"\n",
    "    if dic_result['GS']['ID']:\n",
    "        for f, iD in enumerate(dic_result['GS']['ID']):\n",
    "            sheetid = iD\n",
    "            sheetname = dic_result['GS']['sheetname'][f]\n",
    "            filename = dic_result['GS']['Filename'][f]\n",
    "            url_gs = 'https://docs.google.com/spreadsheets/d/{0}'\n",
    "            url_gs = url_gs.format(sheetid)\n",
    "            cell_gs = template_gs_load.format(url_gs,\n",
    "\t\t\t\t\t\t\t\t\t  sheetid,\n",
    "\t\t\t\t\t\t\t\t\t  sheetname,\n",
    "\t\t\t\t\t\t\t\t\t  filename)\n",
    "\n",
    "            cell_md = template_gs_md.format(filename)\n",
    "            dic_load_data['GS']['markdown'].append(cell_md)\n",
    "            dic_load_data['GS']['code'].append(cell_gs)\n",
    "\n",
    "##### Special treatment for prefix\n",
    "#### 1 find path in GCS with dot. It means the element in\n",
    "#### gcs_contents is a filename\n",
    "#### 2 get the path of the blob, ie remove filename from path\n",
    "#### 3 Drop duplicate path\n",
    "\n",
    "    if dataset_suffix:\n",
    "        regex_dot = r\"[.]\"\n",
    "        regex_slash = r\".*(?=/.)\"\n",
    "        list_blob_dot = []\n",
    "        for bucket in gcs_contents:\n",
    "            for blob in bucket['blob']:\n",
    "                matches = re.search(regex_dot, blob)\n",
    "                if matches:\n",
    "                    matches_ = re.search(regex_slash, blob)\n",
    "                    full_path = '{0}/{1}'\n",
    "                    full_path = full_path.format(bucket['Bucket'], matches_.group())\n",
    "                    list_blob_dot.append(full_path)\n",
    "        list_blob_dot = list(dict.fromkeys(list_blob_dot))\n",
    "\n",
    "#### We now have a list of paths. We will check which one has the preffix\n",
    "        l_dataset = []\n",
    "        for blob in list_blob_dot:\n",
    "            split_ = blob.split(\"/\",  1)\n",
    "            bucket = split_[0]\n",
    "            blob_ = split_[1]\n",
    "            prefix = '{0}/{1}'.format(blob_, dataset_suffix)\n",
    "            l_blob =  gcp.list_blob(bucket = bucket, prefix = prefix)\n",
    "            if l_blob['blob']:\n",
    "                break\n",
    "        for d in l_blob['blob']:\n",
    "            split_ = d.rsplit(\"/\",  1)\n",
    "            l_dataset.append(split_[1])   \n",
    "\n",
    "        template_prefix = \"\"\"\n",
    "    df_load_data = pd.DataFrame()\n",
    "    for dataset in {0}:\n",
    "    gcp.download_blob(bucket_name = '{1}',\n",
    "                  destination_blob_name = '{2}',\n",
    "                  source_file_name = dataset)\n",
    "                  \n",
    "    df_temp = pd.read_csv(dataset,\n",
    "                          compression='gzip',\n",
    "                          header=0,\n",
    "                          sep=',',\n",
    "                          quotechar='\"',\n",
    "                          error_bad_lines=False)\n",
    "    df_load_data = df_load_data.append(df_temp)\n",
    "    df_load_data.head()\n",
    "    \"\"\"\n",
    "\n",
    "#### Append to the dic\n",
    "\n",
    "        ggs_cell = template_prefix.format(l_dataset,bucket, blob_)\n",
    "        ggs_md = template_gcs_md.format(notebook_name)\n",
    "        dic_load_data['GCS']['markdown'].append(ggs_md)\n",
    "        dic_load_data['GCS']['code'].append(ggs_cell)\n",
    "\n",
    "\t### If preffix, then override the final destination for GCP\n",
    "\t### If list dataset has both preffix and not prefix for GCP\n",
    "\t### origin, then preffix overide the destination cell\n",
    "\n",
    "        dic_destination_inf = {\n",
    "\t\t'Bucket':bucket,\n",
    "\t\t'destination_blob_name':blob_,\n",
    "\t\t'new_destination_blob': blob_.replace('Raw_', 'Processed_')\n",
    "\t}\n",
    "\n",
    "### Add markdown origin\n",
    "    template_ds = '\\n - {0}'\n",
    "    l_gcs = []\n",
    "    gs_url = '[{0}](https://docs.google.com/spreadsheets/d/{1})'\n",
    "    for key, value in dic_result.items():\n",
    "        if key == 'BQ':\n",
    "            l_gcs.append('\\n ### Big Query Dataset \\n')\n",
    "            for file in value['Filename']:\n",
    "                cell_gcs = template_ds.format(file)\n",
    "                l_gcs.append(cell_gcs)\n",
    "        elif key == 'GCS':\n",
    "            l_gcs.append('\\n ### Google Cloud Storage Dataset \\n')\n",
    "            for file in value['Filename']:\n",
    "                cell_gcs = template_ds.format(file)\n",
    "                l_gcs.append(cell_gcs)\n",
    "            if dataset_suffix:\n",
    "                cell_gcs = template_ds.format(dataset_suffix)\n",
    "                l_gcs.append(cell_gcs)\n",
    "        else:\n",
    "            l_gcs.append('\\n ### Google Spreadsheet Dataset \\n')\n",
    "            for i, file in enumerate(value['Filename']):\n",
    "                cell_gs = template_ds.format(file)\n",
    "                sheet_name = value['Filename'][i]\n",
    "                sheet_ID = value['ID'][i]\n",
    "                url = gs_url.format(sheet_name, sheet_ID)\n",
    "                cell_gs = template_ds.format(url)\n",
    "                l_gcs.append(cell_gs)\n",
    "    cell_datasource = ' '.join(l_gcs)\n",
    "\n",
    "    nb_cells[1][1] = cell_datasource\n",
    "\n",
    "### Change profiling\n",
    "    nb_cells[9][1][2] = nb_cells[9][1][2].replace(\"NAME\", notebook_name)\n",
    "\n",
    "### Upload to the Cloud\n",
    "    template_upload_bqgcs_md = \"\"\"\n",
    "\n",
    "### Move to GCS and BigQuery\n",
    "\n",
    "We move the dataset to the following:\n",
    "\n",
    "- **bucket**: *{0}*\n",
    "\n",
    "- **Destination_blob**: *{1}*\n",
    "- **name**:  *{2}.gz*\n",
    "- **Dataset**: *{3}*\n",
    "\n",
    "- **table**: *{2}*\n",
    "\n",
    "### GCS\n",
    "\n",
    "We first need to save *{2}* with `.gz` extension locally then we can move it\n",
    "to GCS\n",
    "\"\"\"\n",
    "\n",
    "    template_upload_bqgcs = \"\"\"\n",
    "### First save locally\n",
    "df_final.to_csv(\n",
    "\t'{0}.gz',\n",
    "\tsep=',',\n",
    "\theader=True,\n",
    "\tindex=False,\n",
    "\tchunksize=100000,\n",
    "\tcompression='gzip',\n",
    "\tencoding='utf-8')\n",
    "\n",
    "### Then upload to GCS\n",
    "bucket_name = '{1}'\n",
    "destination_blob_name = '{2}'\n",
    "source_file_name = '{0}.gz'\n",
    "gcp.upload_blob(bucket_name, destination_blob_name, source_file_name)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    template_upload_bq = \"\"\"\n",
    "\n",
    "### Move to bigquery\n",
    "bucket_gcs ='{0}/{1}/{2}.gz'\n",
    "gcp.move_to_bq_autodetect(dataset_name= '{3}',\n",
    "\t\t\t\t\t\t\t name_table= '{2}',\n",
    "\t\t\t\t\t\t\t bucket_gcs=bucket_gcs)\n",
    "\"\"\"\n",
    "\n",
    "    if destination_engine == 'GCP':\n",
    "        mkdown_output = template_upload_bqgcs_md.format(dic_destination_inf['Bucket'],\n",
    "\t\t\t\t\t\t\t dic_destination_inf['new_destination_blob'],\n",
    "\t\t\t\t\t\t\t notebook_name,\n",
    "\t\t\t\t\t\t\t bigquery_dataset\n",
    "\t\t\t\t\t\t\t)\n",
    "        code_output_gcs = template_upload_bqgcs.format(notebook_name,\n",
    "\t\t\t\t\t\t\t dic_destination_inf['Bucket'],\n",
    "\t\t\t\t\t\t\t dic_destination_inf['new_destination_blob']\n",
    "\t\t\t\t\t\t\t)\n",
    "        code_output_bq = template_upload_bq.format(dic_destination_inf['Bucket'],\n",
    "\t\t\t\t\t\t  dic_destination_inf['new_destination_blob'],\n",
    "\t\t\t\t\t\t  notebook_name,\n",
    "\t\t\t\t\t\t  bigquery_dataset\n",
    "\t\t\t\t\t\t )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#### Generate nb\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cell_left = 'cell_{0}'\n",
    "    cell_right = '{0}'\n",
    "\n",
    "\n",
    "    dic_cells={}\n",
    "    for i, cell in enumerate(nb_cells[0:6]):\n",
    "        _source = ''.join(cell[1])\n",
    "        rhs = cell_right.format(_source)\n",
    "\t#lhs = cell_left.format(i)\n",
    "        if cell[0] == 'markdown':\n",
    "            dic_cells[\"markdown_{}\".format(i)] = rhs\n",
    "        elif cell[0] == 'code':\n",
    "            dic_cells[\"code_{}\".format(i)] = rhs\n",
    "\n",
    "### GCS\n",
    "    next_cells_available = len(dic_load_data['GCS']['markdown']) + 6\n",
    "    for i, markdown in enumerate(dic_load_data['GCS']['markdown']):\n",
    "        _md = ''.join(markdown)\n",
    "        code = dic_load_data['GCS']['code'][i]\n",
    "        _code = ''.join(code)\n",
    "\n",
    "        rhs_md = cell_right.format(_md)\n",
    "        rhs_code = cell_right.format(_code)\n",
    "\t#lhs = cell_left.format(i + 3)\n",
    "        dic_cells[\"markdown_{}\".format(i+5)] = rhs_md\n",
    "        dic_cells[\"code_{}\".format(i + 6)] = rhs_code\n",
    "\n",
    "        next_cells_available = next_cells_available + i\n",
    "\n",
    "### GBQ\n",
    "    for i, markdown in enumerate(dic_load_data['BQ']['markdown']):\n",
    "        _md = ''.join(markdown)\n",
    "        code = dic_load_data['BQ']['code'][i]\n",
    "        _code = ''.join(code)\n",
    "\n",
    "        rhs_md = cell_right.format(_md)\n",
    "        rhs_code = cell_right.format(_code)\n",
    "\t#lhs = cell_left.format(i + 3)\n",
    "        dic_cells[\"markdown_{}\".format(i+next_cells_available)] = rhs_md\n",
    "        dic_cells[\"code_{}\".format(i + next_cells_available + 1)] = rhs_code\n",
    "\n",
    "        next_cells_available = next_cells_available + 1\n",
    "    next_cells_available = next_cells_available + 3\n",
    "### GS\n",
    "    for i, markdown in enumerate(dic_load_data['GS']['markdown']):\n",
    "        _md = ''.join(markdown)\n",
    "        code = dic_load_data['GS']['code'][i]\n",
    "        _code = ''.join(code)\n",
    "\n",
    "        rhs_md = cell_right.format(_md)\n",
    "        rhs_code = cell_right.format(_code)\n",
    "\t#lhs = cell_left.format(i + 3)\n",
    "        dic_cells[\"markdown_{}\".format(i + next_cells_available)] = rhs_md\n",
    "        dic_cells[\"code_{}\".format(i + next_cells_available + 1)] = rhs_code\n",
    "\n",
    "        next_cells_available = next_cells_available + 1\n",
    "    next_cells_available = next_cells_available + 5\n",
    "\n",
    "### Add step cells\n",
    "    _source = ''.join(nb_cells[6][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = rhs\n",
    "    dic_cells[\"code_{}\".format(next_cells_available)] = []\n",
    "    next_cells_available = next_cells_available + 2\n",
    "\n",
    "### profiling\n",
    "\n",
    "    _source = ''.join(nb_cells[8][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = rhs\n",
    "    next_cells_available = next_cells_available + 1\n",
    "\n",
    "    _source = ''.join(nb_cells[9][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"code_{}\".format(next_cells_available)] = rhs\n",
    "    next_cells_available = next_cells_available + 1\n",
    "\n",
    "### upload to cloud\n",
    "    _source = ''.join(nb_cells[10][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = rhs\n",
    "    next_cells_available = next_cells_available + 1\n",
    "\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = mkdown_output\n",
    "    dic_cells[\"code_{}\".format(next_cells_available + 1)] = code_output_gcs\n",
    "    dic_cells[\"code_{}\".format(next_cells_available + 2)] = code_output_bq\n",
    "\n",
    "### generate nb\n",
    "    nb['cells'] = []\n",
    "    regex = r\"^markdown\"\n",
    "    for k, v in dic_cells.items():\n",
    "        if re.search(regex, k):\n",
    "            nb['cells'].extend([nbf.v4.new_markdown_cell(v)])\n",
    "        else:\n",
    "            nb['cells'].extend([nbf.v4.new_code_cell(v)])\n",
    "    name_nb = notebook_name + '_preprocessing.ipynb'\n",
    "\n",
    "\n",
    "    #os.chdir(path_)\n",
    "    nbf.write(nb, name_nb)\n",
    "    \n",
    "    print(\"Notebook script {} generated sucessfully.\".format(notebook_name))\n",
    "    project = ''\n",
    "    template_bq_input =\"https://console.cloud.google.com/bigquery?project={0}&p={0}&d={1}&t={2}&page=table\"\n",
    "    bq_input = template_bq_input.format(project, bigquery_dataset, notebook_name)\n",
    "    template_gsutil = \"gs//{0}/{1}/{2}.gz\"\n",
    "    if dataset_suffix:\n",
    "        template_gsutil = template_gsutil.format(dic_destination_inf['Bucket'], \n",
    "        dic_destination_inf['destination_blob_name'], dataset_suffix)\n",
    "    else:\n",
    "        template_gsutil = template_gsutil.format(dic_destination_inf['Bucket'], \n",
    "        dic_destination_inf['destination_blob_name'], 'UNKNOWN')\n",
    "\n",
    "    studio_ =\"auto-nb-studio -u '{0}' -c 'GCP' -n '{1}' -p '{2}' -d 'ADD DATE VAR' -m 'US' -t '/PATH CREDENTIAL'\"\n",
    "    studio_ = studio_.format(username, notebook_name, bigquery_dataset)\n",
    "\n",
    "    print( \"\"\"\n",
    "Please, go to the section Inputs_datasource from\n",
    "    https://coda.io/d/MasterFile-Database_dvfMWDBnHh8/Main_suj76#_lubx_ \\n\n",
    " paste the following information \\n \n",
    " Storage:\n",
    " - Storage: GCS\n",
    " - Top_level: {0}\n",
    " - Path: {1}\n",
    " - Filename: {2}.gz\n",
    " - Description: Add detailed description\n",
    " - Dataset_documentation: If any, add URL\n",
    " - Size: Go to console to get the size\n",
    " - Status: Closed\n",
    " - Source_data:{3}\n",
    " - Link_methodology: If any, add URL\n",
    " - JupyterStudio: Leave unselect\n",
    " - Profiling: Leave unselect \\n\n",
    " Big Query :\n",
    "  - Storage: BQ\n",
    "  - Top_level: {5}\n",
    "  - Path: None\n",
    "  - Filename: {2}\n",
    "  - Source_data:{4}\n",
    "  - JupyterStudio: Use \\n \n",
    " {6} \\n\n",
    "  to create a notebook for the studio, then select the checkbox. Only if Dataset in BigQuery!\n",
    " - Profiling: select the checkbox\n",
    "      \"\"\".format(dic_destination_inf['Bucket'], dic_destination_inf['new_destination_blob'],\n",
    "     notebook_name, template_gsutil, bq_input, bigquery_dataset, studio_)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notebook_analytics(params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    notebook_name = params['notebook_name']\n",
    "    project_name = params['project_name']\n",
    "    input_datasets = params['input_datasets'][0] ### Only one dataset, but its a list\n",
    "    sheetnames = params['sheetnames'][0]### Only one sheet, but its a list\n",
    "    bigquery_dataset = params['bigquery_dataset']\n",
    "    destination_engine = params['destination_engine']\n",
    "    path_ = params['path_analytics']\n",
    "    path_notebook = params['path_notebook_analytics']\n",
    "    project='valid-pagoda-132423'\n",
    "    username = \"thomas\"\n",
    "    pathtoken = params['pathtoken']\n",
    "    connector_en = params['connector']\n",
    "    labels = params['labels']\n",
    "    \n",
    "    with open(path_notebook) as f:\n",
    "        ipynb = json.load(f)\n",
    "        \n",
    "    nb_cells = []\n",
    "    output_str = \"\"\n",
    "\n",
    "# Parse cells in Jupyter notebook template\n",
    "    for cell in ipynb['cells']:\n",
    "        cell_type = cell['cell_type']\n",
    "        source = cell['source']\n",
    "        nb_cells.append([cell_type, source])\n",
    "    name_project = project_name.replace(' ', '_')\n",
    "## 1 Change the header\n",
    "    nb_cells[0][1][0] = nb_cells[0][1][0].replace('TITLE', name_project)\n",
    "    now = datetime.datetime.now()\n",
    "    nb_cells[0][1][2] = nb_cells[0][1][2].replace('XX', now.strftime(\"%Y-%m-%d %H:%M %Z\"))\n",
    "\n",
    "## 2 change connector\n",
    "    if connector_en == 'GS':\n",
    "        nb_cells[4][1][2] = nb_cells[4][1][2].replace(\"()\", \"('GS')\")\n",
    "    else:\n",
    "        nb_cells[4][1][2] = nb_cells[4][1][2].replace(\"()\", \"('GCP')\")\n",
    "\n",
    "# Connect to Google: Token is indicated by the user in the command line\n",
    "    gs = connector.open_connection(online_connection=False,\n",
    "                               path_credential=pathtoken)\n",
    "\n",
    "    if connector_en == 'GS':\n",
    "        service_gd = gs.connect_remote(engine='GS')\n",
    "        gdr = connect_drive.connect_drive(service_gd['GoogleDrive'])\n",
    "    else:\n",
    "        service_gcp = gs.connect_remote(engine='GCP')\n",
    "        gcp = connect_cloud_platform.connect_console(project=project,\n",
    "                                                 service_account=service_gcp['GoogleCloudP'])\n",
    "\n",
    "    if connector_en == 'GS':\n",
    "\n",
    "    \n",
    "        sID = gdr.find_file_id(file_name=input_datasets, to_print=True)\n",
    "        url_spreadsheet = 'https://docs.google.com/spreadsheets/d/' + sID\n",
    "\n",
    "    \n",
    "\n",
    "        print('Loading data, be patient')\n",
    "    ### get n rows and columns\n",
    "        try:\n",
    "            nb_cols, n_row = gdr.getRowAndColumns(sheetID = sID,\n",
    "                     sheetName = sheetnames)  \n",
    "        except:\n",
    "            print('Method 1 failed, trying Method 2')\n",
    "            n_row = gdr.getLatestRow(sID, sheetnames)  \n",
    "            nb_cols = gdr.getColumnNumber(sID, sheetnames)\n",
    "\n",
    "        load_data = gdr.upload_data_from_spreadsheet(sheetID = sID,\n",
    "         sheetName = sheetnames,\n",
    "         to_dataframe = True)\n",
    "\n",
    "        load_data = load_data.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "        cell_upload_template = \"\"\"\n",
    "from GoogleDrivePy.google_drive import connect_drive\n",
    "gdr = connect_drive.connect_drive(service['GoogleDrive'])\n",
    "\n",
    "df_final = gdr.upload_data_from_spreadsheet(sheetID = '{0}',\n",
    "     sheetName = '{1}',\n",
    "     to_dataframe = True)\n",
    "\n",
    "df_final = df_final.apply(pd.to_numeric, errors='ignore')\n",
    "df_final.head()\n",
    "    \"\"\"\n",
    "        cell_upload = cell_upload_template.format(sID, sheetnames)\n",
    "\n",
    "    # Change connector cell\n",
    "        nb_cells[0][1][18] = nb_cells[0][1][18].replace('XXX', input_datasets)\n",
    "        nb_cells[0][1][18] = nb_cells[0][1][18].replace('HERE', url_spreadsheet)\n",
    "\n",
    "        l_dtypes = load_data.dtypes.to_list()\n",
    "\n",
    "    else:\n",
    "\n",
    "    #### Need to find the dataset from the table\n",
    "    ### We are supposed to have it ..\n",
    "\n",
    "        dtasets = gcp.list_dataset()\n",
    "        for d in dtasets['Dataset']:\n",
    "            all_tables = gcp.list_tables(dataset = d)['tables']\n",
    "            if input_datasets in all_tables:\n",
    "                dataset = d\n",
    "\n",
    "        query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM {0}.{1} \"\n",
    "\n",
    "        )\n",
    "\n",
    "        query = query.format(dataset, input_datasets)\n",
    "\n",
    "        print('Loading data, be patient')\n",
    "        load_data = gcp.upload_data_from_bigquery(query = query,\n",
    "         location = 'US')\n",
    "\n",
    "        l_dtypes = load_data.dtypes.to_list()\n",
    "\n",
    "        cell_upload_template = \"\"\"\n",
    "from GoogleDrivePy.google_platform import connect_cloud_platform\n",
    "project = '{2}'\n",
    "gcp = connect_cloud_platform.connect_console(project = project, \n",
    "                                             service_account = service['GoogleCloudP'])    \n",
    "query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM {0}.{1} \"\n",
    "\n",
    "        )\n",
    "\n",
    "df_final = gcp.upload_data_from_bigquery(query = query, location = 'US')\n",
    "df_final.head()\n",
    "    \"\"\"\n",
    "    \n",
    "        cell_upload = cell_upload_template.format(dataset, input_datasets, project[0])\n",
    "        nb_cells[0][1][18] = nb_cells[0][1][18].replace('XXX', input_datasets)\n",
    "    \n",
    "    nb_cells[5][1] = cell_upload\n",
    "\n",
    "    ### Set if label exists\n",
    "    if labels:\n",
    "\n",
    "        if len(labels) != len(l_dtypes):\n",
    "            print('ERROR: Data source has {0} variables and you input {1} labels'.format(\n",
    "            len(l_dtypes), len(labels)\n",
    "    ))\n",
    "            sys.exit()\n",
    "\n",
    "    else:\n",
    "        labels = list(load_data)\n",
    "\n",
    "\n",
    "    dic_name = {'Variables':  list(load_data),\n",
    "           'Labels' : labels,\n",
    "           'Types': l_dtypes}\n",
    "    df_ = pd.DataFrame(dic_name, index = None)\n",
    "    df_html = df_.to_html()\n",
    "    nb_cells[1][1] = df_html\n",
    "\n",
    "    template_token = '{0}/'\n",
    "\n",
    "    nb_cells[4][1][1] = nb_cells[4][1][1].replace('pathtoken', template_token.format(pathtoken))\n",
    "\n",
    "## generate notebook\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cell_left = 'cell_{0}'\n",
    "    cell_right = '{0}'\n",
    "\n",
    "    dic_cells={}\n",
    "    for i, cell in enumerate(nb_cells):\n",
    "        _source = ''.join(cell[1])\n",
    "        rhs = cell_right.format(_source)\n",
    "        lhs = cell_left.format(i)\n",
    "        if cell[0] == 'markdown':\n",
    "            dic_cells[\"markdown_{}\".format(i)] = rhs\n",
    "        elif cell[0] == 'code':\n",
    "            dic_cells[\"code_{}\".format(i)] = rhs\n",
    "\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    nb['cells'] = []\n",
    "    regex = r\"^markdown\"\n",
    "    for k, v in dic_cells.items():\n",
    "        if re.search(regex, k):\n",
    "            nb['cells'].extend([nbf.v4.new_markdown_cell(v)])\n",
    "        else:\n",
    "            nb['cells'].extend([nbf.v4.new_code_cell(v)])\n",
    "    name_nb = name_project + '_analysis.ipynb'\n",
    "    nbf.write(nb, name_nb)\n",
    "    \n",
    "    print(\"Notebook {} generated sucessfully.\".format(project_name))\n",
    "\n",
    "    jup_lab_cl = \"jupyter-lab-launcher -u '{0}' -p '{1}'\".format(username, project_name)\n",
    "\n",
    "    print(\"\"\"\n",
    "    Please, go to section Inputs_projects from\n",
    " https://coda.io/d/MasterFile-Database_dvfMWDBnHh8/Main_suj76#_lubx_ \\n\n",
    " paste the following information in Input Project_notebook \\n \n",
    "\n",
    " - Project_name: {0}\n",
    " - DatasetName: {1}\n",
    " - JupyterAnalysis: {2}\n",
    " - ThirdpartyTool: If AWS Image or any other VM, paste URL\n",
    "\n",
    " \"\"\".format(project,input_datasets,  jup_lab_cl)\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_notebook_studio(params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    notebook_name = params['notebook_name']\n",
    "    project_name = params['project_name']\n",
    "    input_datasets = params['input_datasets'][0] ### Only one dataset, but its a list\n",
    "    sheetnames = params['sheetnames'][0]### Only one sheet, but its a list\n",
    "    bigquery_dataset = params['bigquery_dataset']\n",
    "    destination_engine = params['destination_engine']\n",
    "    path_ = params['path_analytics']\n",
    "    path_notebook = params['path_notebook_studio']\n",
    "    project='valid-pagoda-132423'\n",
    "    username = \"thomas\"\n",
    "    pathtoken = params['pathtoken']\n",
    "    connector_en = params['connector']\n",
    "    labels = params['labels']\n",
    "    date_var = params['date_var']\n",
    "    \n",
    "    with open(path_notebook) as f:\n",
    "        ipynb = json.load(f)\n",
    "        \n",
    "    nb_cells = []\n",
    "    output_str = \"\"\n",
    "    \n",
    "# Parse cells in Jupyter notebook template\n",
    "    for cell in ipynb['cells']:\n",
    "        cell_type = cell['cell_type']\n",
    "        source = cell['source']\n",
    "        nb_cells.append([cell_type, source])\n",
    "    name_project = project_name.replace(' ', '_')\n",
    "## 1 Change the header\n",
    "    nb_cells[0][1][0] = nb_cells[0][1][0].replace('TITLE', name_project)\n",
    "    now = datetime.datetime.now()\n",
    "    nb_cells[0][1][2] = nb_cells[0][1][2].replace('XX', now.strftime(\"%Y-%m-%d %H:%M %Z\"))\n",
    "\n",
    "## 2 change connector\n",
    "    if connector_en == 'GS':\n",
    "        nb_cells[5][1][1] = nb_cells[5][1][1].replace(\"()\", \"('GS')\")\n",
    "    else:\n",
    "        nb_cells[5][1][1] = nb_cells[5][1][1].replace(\"()\", \"('GCP')\")\n",
    "\n",
    "## Connect to Google: Token is indicated by the user in the command line\n",
    "    gs = connector.open_connection(online_connection = False, \n",
    "        path_credential = pathtoken)\n",
    "    if connector_en == 'GS':\n",
    "        service_gd = gs.connect_remote(engine = 'GS')\n",
    "        gdr = connect_drive.connect_drive(service_gd['GoogleDrive'])\n",
    "    else: \n",
    "        service_gcp = gs.connect_remote(engine = 'GCP')\n",
    "        gcp = connect_cloud_platform.connect_console(project = project, \n",
    "                                             service_account = service_gcp['GoogleCloudP'])\n",
    "\n",
    "### Change cell data source, var name and load data\n",
    "\n",
    "    if connector_en == 'GS':\n",
    "\n",
    "    \n",
    "        sID = gdr.find_file_id(file_name=input_datasets, to_print=True)\n",
    "        url_spreadsheet = 'https://docs.google.com/spreadsheets/d/' + sID\n",
    "\n",
    "    \n",
    "\n",
    "        print('Loading data, be patient')\n",
    "    ### get n rows and columns\n",
    "        try:\n",
    "            nb_cols, n_row = gdr.getRowAndColumns(sheetID = sID,\n",
    "                     sheetName = sheetnames)  \n",
    "        except:\n",
    "            print('Method 1 failed, trying Method 2')\n",
    "            n_row = gdr.getLatestRow(sID, sheetnames)  \n",
    "            nb_cols = gdr.getColumnNumber(sID, sheetnames)\n",
    "\n",
    "        load_data = gdr.upload_data_from_spreadsheet(sheetID = sID,\n",
    "         sheetName = sheetnames,\n",
    "         to_dataframe = True)\n",
    "\n",
    "        load_data = load_data.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "        cell_upload_template = \"\"\"\n",
    "from GoogleDrivePy.google_drive import connect_drive\n",
    "gdr = connect_drive.connect_drive(service['GoogleDrive'])\n",
    "\n",
    "df_final = gdr.upload_data_from_spreadsheet(sheetID = '{0}',\n",
    "     sheetName = '{1}',\n",
    "     to_dataframe = True)\n",
    "\n",
    "df_final = df_final.apply(pd.to_numeric, errors='ignore')\n",
    "df_final.head()\n",
    "    \"\"\"\n",
    "        cell_upload = cell_upload_template.format(sID, sheetnames)\n",
    "\n",
    "    # Change connector cell\n",
    "        nb_cells[6][1][4] = nb_cells[6][1][4].replace('XXX', input_datasets)\n",
    "        nb_cells[6][1][4] = nb_cells[6][1][4].replace('HERE', url_spreadsheet)\n",
    "\n",
    "        l_dtypes = load_data.dtypes.to_list()\n",
    "\n",
    "    else:\n",
    "\n",
    "    #### Need to find the dataset from the table\n",
    "\n",
    "        dtasets = gcp.list_dataset()\n",
    "        for d in dtasets['Dataset']:\n",
    "            all_tables = gcp.list_tables(dataset = d)['tables']\n",
    "            if input_datasets in all_tables:\n",
    "                dataset = d\n",
    "\n",
    "        query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM {0}.{1} \"\n",
    "\n",
    "        )\n",
    "\n",
    "        query = query.format(dataset, input_datasets)\n",
    "\n",
    "        print('Loading data, be patient')\n",
    "        load_data = gcp.upload_data_from_bigquery(query = query,\n",
    "         location = 'US')\n",
    "\n",
    "        l_dtypes = load_data.dtypes.to_list()\n",
    "\n",
    "        cell_upload_template = \"\"\"\n",
    "from GoogleDrivePy.google_platform import connect_cloud_platform\n",
    "project = '{2}'\n",
    "gcp = connect_cloud_platform.connect_console(project = project, \n",
    "                                             service_account = service['GoogleCloudP'])    \n",
    "query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM {0}.{1} \"\n",
    "\n",
    "        )\n",
    "\n",
    "df_final = gcp.upload_data_from_bigquery(query = query, location = 'US')\n",
    "df_final.head()\n",
    "    \"\"\"\n",
    "        cell_upload = cell_upload_template.format(dataset, input_datasets, project[0])\n",
    "        nb_cells[6][1][4] = nb_cells[6][1][4].replace('XXX', input_datasets)\n",
    "\n",
    "    nb_cells[9][1] = cell_upload\n",
    "\n",
    "### Set if label exists\n",
    "    if labels:\n",
    "\n",
    "        if len(labels) != len(l_dtypes):\n",
    "            print('ERROR: Data source has {0} variables and you input {1} labels'.format(\n",
    "            len(l_dtypes), len(labels)\n",
    "    ))\n",
    "            sys.exit()\n",
    "\n",
    "    else:\n",
    "        labels = list(load_data)\n",
    "\n",
    "\n",
    "    dic_name = {'Variables':  list(load_data),\n",
    "           'Labels' : labels,\n",
    "           'Types': l_dtypes}\n",
    "    df_ = pd.DataFrame(dic_name, index = None)\n",
    "    df_html = df_.to_html()\n",
    "    nb_cells[7][1] = df_html\n",
    "\n",
    "    ## Change Year\n",
    "    nb_cells[14][1][4] = nb_cells[14][1][4].replace('CHANGE VARIABLE', date_var)\n",
    "\n",
    "    ## generate notebook\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cell_left = 'cell_{0}'\n",
    "    cell_right = '{0}'\n",
    "\n",
    "    dic_cells={}\n",
    "    for i, cell in enumerate(nb_cells):\n",
    "        _source = ''.join(cell[1])\n",
    "        rhs = cell_right.format(_source)\n",
    "        lhs = cell_left.format(i)\n",
    "        if cell[0] == 'markdown':\n",
    "            dic_cells[\"markdown_{}\".format(i)] = rhs\n",
    "        elif cell[0] == 'code':\n",
    "            dic_cells[\"code_{}\".format(i)] = rhs\n",
    "\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    nb['cells'] = []\n",
    "    regex = r\"^markdown\"\n",
    "    for k, v in dic_cells.items():\n",
    "        if re.search(regex, k):\n",
    "            nb['cells'].extend([nbf.v4.new_markdown_cell(v)])\n",
    "        else:\n",
    "            nb['cells'].extend([nbf.v4.new_code_cell(v)])\n",
    "    name_nb = name_project + '_studio.ipynb'\n",
    "    nbf.write(nb, name_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "butt_act = widgets.Button(description='Add table')\n",
    "outt_act = widgets.Output()\n",
    "\n",
    "\n",
    "def on_butt_clicked_act(_):\n",
    "    #outt.clear_output()\n",
    "    \n",
    "    list_datasets = ast.literal_eval(spread_text.value)\n",
    "    tables_bq = ast.literal_eval(table_text.value)\n",
    "    \n",
    "    list_datasets.extend(tables_bq)\n",
    "    while(\"\" in list_datasets) : \n",
    "        list_datasets.remove(\"\")\n",
    "    \n",
    "    dic_ ={\n",
    "    'notebook_name' : text.value,\n",
    "    'connector':connector_engine.value,   \n",
    "    'project_name' : dropdown_prject.value,\n",
    "    'input_datasets' : list_datasets,\n",
    "    'sheetnames' : ast.literal_eval(sheet_text.value),\n",
    "    'bigquery_dataset' : ast.literal_eval(dataset_text.value),\n",
    "    'destination_engine' : 'GCP',\n",
    "    'name_nb' :'{}_preprocessing.ipynb'.format(text.value),\n",
    "    'path_processing':os.path.join(path.value,dropdown_prject.value, 'Data_preprocessing'),\n",
    "    'path_analytics':os.path.join(path.value,dropdown_prject.value, 'Data_analysis'),    \n",
    "    'path_studio':os.path.join(path.value,dropdown_prject.value, 'Notebooks_Ready_to_use_studio'),  \n",
    "    'path_notebook_processing' : os.path.join(path.value,dropdown_prject.value,\n",
    "                                   'Data_preprocessing',\n",
    "                                   'Template_preprocessing.ipynb'),\n",
    "    'path_notebook_analytics':os.path.join(path.value,dropdown_prject.value,\n",
    "                              'Data_analysis',\n",
    "                              'Template_analysis.ipynb'\n",
    "                             ),\n",
    "    'path_notebook_studio':os.path.join(path.value,dropdown_prject.value,\n",
    "                              'Notebooks_Ready_to_use_studio',\n",
    "                              'Template_studio.ipynb'\n",
    "                             ),\n",
    "    'pathtoken':creds.value,\n",
    "     'labels':labels.value,\n",
    "        'date_var':date_var.value\n",
    "        \n",
    "    }\n",
    "    \n",
    "    if menu.value == \"Connect_source\":\n",
    "        os.chdir(dic_['path_processing'])\n",
    "        print('Connect_source')\n",
    "        #print('Begin to create Notebook {}'.format(dic_['notebook_name']))\n",
    "        #generate_notebook_processing(dic_)\n",
    "    elif menu.value == \"Analytics\":\n",
    "        \n",
    "        os.chdir(dic_['path_analytics'])\n",
    "        print(\"Analytics\")\n",
    "        generate_notebook_analytics(params= dic_)\n",
    "    else:\n",
    "        os.chdir(dic_['path_studio'])\n",
    "        print(\"Studio\")\n",
    "        generate_notebook_studio(params=dic_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Create Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butt_act.on_click(on_butt_clicked_act)\n",
    "box = widgets.VBox([menu,connector_engine,text, path, dropdown_prject, creds,\n",
    "                    spread_text, sheet_text,\n",
    "                    dataset_text, table_text,date_var,  labels, butt_act,outt_act])\n",
    "butt.on_click(on_butt_clicked)\n",
    "# display\n",
    "data_spread = widgets.VBox([spreadW, sheetW,butt,outt])\n",
    "butt1.on_click(on_butt_clicked1)\n",
    "# display\n",
    "data_table = widgets.VBox([datasetsdW, tablesW,butt1,outt1])\n",
    "children = [box, data_spread, data_table]\n",
    "# initializing a tab\n",
    "tab = widgets.Tab()\n",
    "# setting the tab windows \n",
    "tab.children = children\n",
    "# changing the title of the first and second window\n",
    "tab.set_title(0, 'Parameters')\n",
    "tab.set_title(1, 'Select Spreadsheets')\n",
    "tab.set_title(2, 'Select Tables')\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create Transfert Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "creds = widgets.Text(\n",
    "       value='/Users/Thomas/Google Drive/Projects/Data_science/Google_code_n_Oauth/Client_Oauth/Google_auth/',\n",
    "       description='path creds Google', )\n",
    "\n",
    "# Date var -> srtudio\n",
    "excel_path = widgets.Text(\n",
    "       description='Path to Excel file', \n",
    "    layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Prepare function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##### Function to run the program\n",
    "def prepare_transfert(nameFile, gdr = None, move_to_drive = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ### Get topic\n",
    "    #regex = r\"\\.[^.]*$\"\n",
    "    #topic = re.sub(regex, '', nameFile)\n",
    "    topic = os.path.splitext(os.path.basename(nameFile))[0]\n",
    "    \n",
    "    ### Load excel\n",
    "\n",
    "    print('Loading {}'.format(topic))\n",
    "    df = pd.read_excel(nameFile)\n",
    "    \n",
    "    ### Add vars\n",
    "    df['Topic'] = topic\n",
    "    date_today = pd.Timestamp.today()\n",
    "    df['Date_added'] = date_today\n",
    "    df['Date_added'] = df['Date_added'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    ### Drop useless vars\n",
    "    df = df.drop(columns = ['Highlight Color', 'Highlight Color Code'])\n",
    "    \n",
    "    ### Reorder\n",
    "    reorder = ['Topic', 'Date', 'Date_added', 'Website Title',\n",
    "           'URL', 'Color Category', 'Highlighted Text', 'Note']\n",
    "\n",
    "    df = df[reorder]\n",
    "\n",
    "    na_note = df['Note'].isna().sum()\n",
    "    if na_note != 0:\n",
    "        df = df.fillna(\"#{}\".format(topic))\n",
    "        print(\"\"\"\n",
    "        There are {} highlights without tags over {}. \n",
    "        It is about {:.0%} of the total\n",
    "        Each empty highlight is filled by #{}\n",
    "        \"\"\".format(na_note,df.shape[0],na_note/df.shape[0], topic))\n",
    "    \n",
    "    ### Compare with existing data\n",
    "    #### Always this spreadsheet\n",
    "    \n",
    "    sheetID = '1B5hi8fKckMY15BA3g1a9t7wBtG1IEJiw5mmyKS1Exwc'\n",
    "    sheetName = 'MetaData'\n",
    "    df_master = gdr.upload_data_from_spreadsheet(sheetID,\n",
    "                                                 sheetName,\n",
    "                                                 to_dataframe = True)\n",
    "    \n",
    "    #### Extract only same topic\n",
    "    \n",
    "    df_master =  df_master[df_master['Topic'] == topic]\n",
    "    \n",
    "    ### Return from df only new highlights\n",
    "    \n",
    "    toCompare = df_master['Highlighted Text'].tolist()\n",
    "    new_highlights = df[~df['Highlighted Text'].isin(toCompare)]\n",
    "    \n",
    "    if new_highlights.empty:\n",
    "        print('The Excel file {0} does not contain new highlights'.format(nameFile))\n",
    "        exit()\n",
    "    else:\n",
    "        n_newHighlights = new_highlights.shape[0]\n",
    "        print(\"\"\"\n",
    "    {} new highlights will be added to MasterFile and Dynalist.\n",
    "    Feel free to move to Dynalist the new highlights and change the Status to\n",
    "    Transfered \"\"\".format(n_newHighlights))\n",
    "    #### Reshape to Dynalist\n",
    "    print('The spreadsheet is availab here \\n \\\n",
    "    https://docs.google.com/spreadsheets/d/1B5hi8fKckMY15BA3g1a9t7wBtG1IEJiw5mmyKS1Exwc')\n",
    "    \n",
    "    col_needed = ['Highlighted Text' , 'Website Title', 'Note', 'Color Category']\n",
    "    df_dynalist = new_highlights.copy()\n",
    "    df_dynalist = df_dynalist[col_needed]\n",
    "    df_dynalist['ID'] = df_dynalist['Website Title'\n",
    "                                 ].rank(\n",
    "    method='dense').astype(int)\n",
    "    df_dynalist = df_dynalist.set_index(['ID', 'Note', 'Color Category'])\n",
    "    df_dynalist = df_dynalist.stack().reset_index()\n",
    "    df_dynalist = df_dynalist.rename(\n",
    "    columns={\n",
    "             'level_3' : 'origin',\n",
    "             0: 'highlights'})\n",
    "    df_dynalist['Topic'] = topic\n",
    "    df_dynalist['Date_added'] = date_today\n",
    "    df_dynalist['Date_added'] = df_dynalist['Date_added'].dt.strftime('%Y-%m-%d')\n",
    "    df_dynalist['status'] = 'Pending'\n",
    "    reorder = ['Topic', 'Date_added', 'ID', 'Color Category', 'Note', 'origin',\n",
    "               'highlights', 'status']\n",
    "    \n",
    "    df_dynalist = df_dynalist[reorder]\n",
    "    \n",
    "    #### Move to Google Spreadsheet\n",
    "    if move_to_drive:\n",
    "        gdr.add_data_to_spreadsheet(data = new_highlights,\n",
    "                        sheetID = sheetID,\n",
    "                        sheetName = sheetName,\n",
    "                        detectRange = True,\n",
    "                        rangeData = None)\n",
    "    \n",
    "        gdr.add_data_to_spreadsheet(data = df_dynalist,\n",
    "                        sheetID = sheetID,\n",
    "                        sheetName = 'Transfert_knowledge',\n",
    "                        detectRange = True,\n",
    "                        rangeData = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "butt_knowledge = widgets.Button(description='Add knowledge')\n",
    "outt_knowledge = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def on_butt_clicked_knowledge(_):\n",
    "    gs = connector.open_connection(online_connection = False, \n",
    "    path_credential = creds.value)\n",
    "\n",
    "    service_gd = gs.connect_remote(engine = 'GS')\n",
    "    gdr = connect_drive.connect_drive(service_gd['GoogleDrive'])\n",
    "\n",
    "    n_newHighlights = prepare_transfert(excel_path.value,\n",
    "                                        gdr = gdr,\n",
    "                                        move_to_drive = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Transfer knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_knowledge = widgets.VBox([creds, excel_path,butt_knowledge,outt_knowledge])\n",
    "butt_knowledge.on_click(on_butt_clicked_knowledge)\n",
    "data_knowledge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
