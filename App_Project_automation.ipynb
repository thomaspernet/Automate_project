{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# App "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import display\n",
    "import re, json, datetime, argparse, sys, os, ast, shutil\n",
    "import nbformat as nbf\n",
    "from Fast_connectCloud import connector\n",
    "from GoogleDrivePy.google_drive import connect_drive\n",
    "from GoogleDrivePy.google_platform import connect_cloud_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+git://github.com/thomaspernet/GoogleDrive-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "# Create Project and Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "name_project = widgets.Text(\n",
    "       value=\"Find an Awesome name\",\n",
    "       description='Project name', layout=Layout(width='50%', height='80px') )\n",
    "\n",
    "path_src = widgets.Text(\n",
    "       value=\"/Users/thomas/Google Drive/Projects/Data_science/GitHub/\" \\\n",
    "\"Template_project_Github\",\n",
    "       description='path Projects', layout=Layout(width='50%', height='80px') )\n",
    "path_dest = widgets.Text(\n",
    "       value='/Users/thomas/Google Drive/Projects/Data_science/GitHub/Repositories',\n",
    "       description='Destination Projects', layout=Layout(width='50%', height='80px') )\n",
    "checkbox = widgets.Checkbox(\n",
    "           description='Create Github repo',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "butt_git = widgets.Button(description='Add Project')\n",
    "outt_git = widgets.Output()\n",
    "\n",
    "def on_butt_clicked_git(_):\n",
    "    print('Begin Create project')\n",
    "    new_path = os.path.join(path_dest.value, name_project.value)\n",
    "    os.mkdir(path=new_path)\n",
    "\n",
    "    def copytree(src, dst, symlinks=False, ignore=None):\n",
    "        for item in os.listdir(src):\n",
    "            s = os.path.join(src, item)\n",
    "            d = os.path.join(dst, item)\n",
    "            if os.path.isdir(s):\n",
    "                shutil.copytree(s, d, symlinks, ignore)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "\n",
    "    copytree(path_src.value,new_path)\n",
    "\n",
    "    ### Rename project\n",
    "    with open(os.path.join(new_path, 'README.md'), \"r\") as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    lines = lines.replace('PROJECTNAME',\n",
    "                          name_project.value)\n",
    "\n",
    "    with open(os.path.join(new_path, 'README.md'), 'w') as file:\n",
    "        file.write(lines)\n",
    "        \n",
    "    print('Project Created here: {}'.format(path_dest.value))\n",
    "    \n",
    "    if checkbox.value:\n",
    "        os.chdir(new_path)\n",
    "\n",
    "    ### Config the hub file\n",
    "    ### https://hub.github.com/ -> for mac\n",
    "    ####https://github.com/github/hub/issues/1067#issuecomment-482133220\n",
    "\n",
    "    os.system(\"git init\")\n",
    "    os.system(\"git add .\")\n",
    "    os.system('git commit -m \"Create proejct\"')\n",
    "    ### Create repo github\n",
    "    os.system('hub create')\n",
    "    ### Oush to GitHub\n",
    "    push =\"git remote add origin https://github.com/thomaspernet/{}\".format(\n",
    "    name_project.value)\n",
    "    os.system(push)\n",
    "    os.system(\"git push -u origin master\")\n",
    "    print('Project {0} added to github. Please open at https://github.com/thomaspernet/{0}'.format(\n",
    "        name_project.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Create project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a305c2984196404cb590cd7a54b04053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Find an Awesome name', description='Project name', layout=Layout(height='80px', widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "butt_git.on_click(on_butt_clicked_git)\n",
    "box = widgets.VBox([name_project,\n",
    "                    path_src,\n",
    "                    path_dest,\n",
    "                    checkbox,\n",
    "                    butt_git,outt_git])\n",
    "box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Create Notebook Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ec08e86dd54559a50157333a6d8336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select:', options=('Connect_source', 'Analytics', 'Studio'), value='Connect_source')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Objective\n",
    "menu = widgets.Dropdown(\n",
    "       options=['Connect_source', 'Analytics', 'Studio'],\n",
    "       value='Connect_source',\n",
    "       description='Select:')\n",
    "menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some widgets\n",
    "text = widgets.Text(\n",
    "       value='My First Notebook',\n",
    "       description='Notebook Name', layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = widgets.Text(\n",
    "       value='/Users/thomas/Google Drive/Projects/Data_science/GitHub/Repositories',\n",
    "       description='path Projects', layout=Layout(width='50%', height='80px') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = os.listdir(path.value)\n",
    "list_project = []\n",
    "for name in top:\n",
    "    list_project.append(name)\n",
    "output_prject = widgets.Output()\n",
    "dropdown_prject = widgets.Dropdown(options =list_project,\n",
    "                                  description='Project name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create list spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "creds = widgets.Text(\n",
    "       value='/Users/Thomas/Google Drive/Projects/Data_science/Google_code_n_Oauth/Client_Oauth/Google_auth/',\n",
    "       description='path creds Google', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Google Drive and Docs, Sheet are now connected. \n",
      "Service Google Drive is stored as <googleapiclient.discovery.Resource object at 0x124a5a0d0> and accessible with \"drive\" \n",
      "Service Google Doc is stored as <googleapiclient.discovery.Resource object at 0x126e961d0> and accessible with \"doc\" \n",
      "Service Google Sheet is stored as <googleapiclient.discovery.Resource object at 0x126f9c290>and accessible with \"sheet\"\n",
      "Service account storage and Bigquery are now connected. \n",
      "Service account storage is stored as <google.cloud.storage.client.Client object at 0x126f9c4d0> and accessible with \"Storage_account\" \n",
      "Service account Bigquery is stored as <google.cloud.bigquery.client.Client object at 0x126f9c710> and accessible with \"bigquery_account\"\n"
     ]
    }
   ],
   "source": [
    "gs = connector.open_connection(online_connection=False,\n",
    "                               path_credential=creds.value)\n",
    "\n",
    "service_gd = gs.connect_remote(engine='GS')\n",
    "service_gcp = gs.connect_remote(engine='GCP')\n",
    "\n",
    "gdr = connect_drive.connect_drive(service_gd['GoogleDrive'])\n",
    "\n",
    "gcp = connect_cloud_platform.connect_console(project='valid-pagoda-132423',\n",
    "                                             service_account=service_gcp['GoogleCloudP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### dropdown spreadsheet\n",
    "sheetid = '1fB-h1Sbdy7wkUghQTugYKBbvNS5plFCc3TCK8zWe71M'\n",
    "sheetname = 'data'\n",
    "\n",
    "spreadsheets = gdr.upload_data_from_spreadsheet(sheetID = sheetid,\n",
    "sheetName = sheetname,to_dataframe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### recherche spreadsheet\n",
    "def findspreadsheet(spreadsheet_name):\n",
    "    sheetID = gdr.find_file_id(file_name = spreadsheet_name,\n",
    "                           to_print = False)\n",
    "    \n",
    "    list_ = gdr.listSpreadsheet(sheetID= sheetID)\n",
    "    \n",
    "    return list_\n",
    "\n",
    "#### Spreadsheet\n",
    "def return_spread(theme, module#, fournisseur\n",
    "             ):\n",
    "    dic_ = {\n",
    "        'spreadsheet':theme,\n",
    "        'sheet':module\n",
    "    }\n",
    "    return dic_\n",
    "\n",
    "# Spreadsheet\n",
    "unique_spreadsheet = (spreadsheets.loc[lambda x: \n",
    "                                 x['Storage'].isin(['GS'])]['Filename']\n",
    "                .sort_values().to_list())\n",
    "spreadW = widgets.Dropdown(options=unique_spreadsheet)\n",
    "init = spreadW.value\n",
    "\n",
    "### Sheet\n",
    "sheetW = widgets.Dropdown(options=\n",
    "                           findspreadsheet(init),\n",
    "                          description = 'Select Sheet'\n",
    "                          )\n",
    "def select_sheet(theme):\n",
    "    sheetW.options = findspreadsheet(theme)\n",
    "\n",
    "\n",
    "i = widgets.interactive(select_sheet, theme=spreadW)\n",
    "\n",
    "#j = widgets.interactive(\n",
    "#    return_spread,\n",
    "#    theme=spreadW,\n",
    "#    module=sheetW,\n",
    "    #fournisseur=fournisseurtW\n",
    "#)\n",
    "butt = widgets.Button(description='Add Spreadsheets')\n",
    "outt = widgets.Output()\n",
    "input_datasets = []\n",
    "sheetnames = []\n",
    "def on_butt_clicked(_):\n",
    "    #outt.clear_output()\n",
    "    with outt:\n",
    "        dic_ = return_spread(spreadW.value, sheetW.value)\n",
    "        input_datasets.append(dic_['spreadsheet'])\n",
    "        sheetnames.append(dic_['sheet'])\n",
    "        \n",
    "        display(input_datasets, sheetnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spread_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list Spreadsheet', layout=Layout(width='50%', height='80px'))\n",
    "sheet_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list sheet', layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create list Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### recherche tabel\n",
    "def findtable(dataset):\n",
    "    \n",
    "    list_ = gcp.list_tables(dataset = dataset)['tables']\n",
    "    \n",
    "    return list_\n",
    "\n",
    "#### table\n",
    "def return_table(theme, module#, fournisseur\n",
    "             ):\n",
    "    dic_ = {\n",
    "        'dataset':theme,\n",
    "        'table':module\n",
    "    }\n",
    "    return dic_\n",
    "\n",
    "# Theme\n",
    "unique_dataset = gcp.list_dataset()['Dataset']\n",
    "datasetsdW = widgets.Dropdown(options=unique_dataset)\n",
    "init = datasetsdW.value\n",
    "\n",
    "### Theme: Module\n",
    "\n",
    "tablesW = widgets.Dropdown(options=\n",
    "                           findtable(init),\n",
    "                           description = 'Select Table'\n",
    "                          )\n",
    "init2 = tablesW.value\n",
    "\n",
    "\n",
    "def select_table(theme):\n",
    "    tablesW.options = findtable(theme)\n",
    "\n",
    "i = widgets.interactive(select_table, theme=datasetsdW)\n",
    "\n",
    "#j = widgets.interactive(\n",
    "#    return_spread,\n",
    "#    theme=spreadW,\n",
    "#    module=sheetW,\n",
    "    #fournisseur=fournisseurtW\n",
    "#)\n",
    "butt1 = widgets.Button(description='Add table')\n",
    "outt1 = widgets.Output()\n",
    "input_dataset = []\n",
    "tables = []\n",
    "\n",
    "def on_butt_clicked1(_):\n",
    "    #outt.clear_output()\n",
    "    with outt1:\n",
    "        dic_ = return_table(datasetsdW.value, tablesW.value)\n",
    "        input_dataset.append(dic_['dataset'])\n",
    "        tables.append(dic_['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list Dataset', layout=Layout(width='50%', height='80px'))\n",
    "table_text = widgets.Text(\n",
    "            value='[\"\"]',\n",
    "            description='list Table', layout=Layout(width='50%', height='80px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_notebook(params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    notebook_name = params['notebook_name']\n",
    "    project_name = params['project_name']\n",
    "    input_datasets = params['input_datasets']\n",
    "    sheetnames = params['sheetnames']\n",
    "    bigquery_dataset = params['bigquery_dataset']\n",
    "    destination_engine = params['destination_engine']\n",
    "    path_ = params['path']\n",
    "    path_notebook = params['path_notebook']\n",
    "    project='valid-pagoda-132423'\n",
    "    username = \"thomas\"\n",
    "    \n",
    "    with open(path_notebook) as f:\n",
    "        ipynb = json.load(f)\n",
    "\n",
    "    nb_cells = []\n",
    "    output_str = \"\"\n",
    "\n",
    "# Parse cells in Jupyter notebook template\n",
    "    for cell in ipynb['cells']:\n",
    "        cell_type = cell['cell_type']\n",
    "        source = cell['source']\n",
    "        nb_cells.append([cell_type, source])\n",
    "\n",
    "## Change header\n",
    "    nb_cells[0][1][0] = nb_cells[0][1][0].replace('TITLE', notebook_name)\n",
    "    now = datetime.datetime.now()\n",
    "    nb_cells[0][1][2] = nb_cells[0][1][2].replace('XX',\n",
    "     now.strftime(\"%Y-%m-%d %H:%M %Z\"))\n",
    "\n",
    "### Add Project name \n",
    "\n",
    "    nb_cells[5][1][8] = nb_cells[5][1][8].replace('PROJECTNAME', project_name) ### Pagoda\n",
    "\n",
    "##### Find if prefix in list of dtaset\n",
    "###### If dataset found, then remove from list, and make\n",
    "##### special variable for special treatment\n",
    "##### So far only one preffix dataset can be added\n",
    "\n",
    "    regex = r\"\\*\"\n",
    "    for d in input_datasets:\n",
    "        matches = re.search(regex, d)\n",
    "        if matches:\n",
    "            dataset_suffix = d.replace(\"*\", \"\")\n",
    "            input_datasets.remove(d)\n",
    "        else:\n",
    "            try:\n",
    "                if dataset_suffix:\n",
    "                    pass\n",
    "            except:\n",
    "                dataset_suffix = False\n",
    "\n",
    "## Add data source\n",
    "## Load dataset\n",
    "    datasets = gcp.list_dataset()\n",
    "    bq_contents = []\n",
    "### To recover the index of the sheetname in sheetname\n",
    "\n",
    "    for key, value in datasets.items():\n",
    "        for dataset in value:\n",
    "            tables = gcp.list_tables(dataset = dataset)\n",
    "            bq_contents.append(tables)\n",
    "## Load Buckets\n",
    "    buckets = gcp.list_bucket()\n",
    "    gcs_contents = []\n",
    "    for key, value in buckets.items():\n",
    "        for bucket in value:\n",
    "            blob = gcp.list_blob(bucket = bucket)\n",
    "            gcs_contents.append(blob)\n",
    "\n",
    "            dic_result = {\n",
    "                'BQ': {'Filename': []},\n",
    "                'GCS': {'Filename': []},\n",
    "                'GS': {'Filename': [],\n",
    "                       'ID': [],\n",
    "                       'sheetname':[]},\n",
    "            }\n",
    "            sheetindex = 0\n",
    "\n",
    "            for l_dataset in input_datasets:\n",
    "                found = False\n",
    "                ### Search in BQ\n",
    "                for i in range(0, len(bq_contents)):\n",
    "                    regex = r'(\\w*{0}\\w*)'.format(l_dataset)\n",
    "                    matches = re.search(regex, str(bq_contents[i]))\n",
    "                    if matches:\n",
    "\t\t\t\t\t# print(l_dataset)\n",
    "                        dic_result['BQ']['Filename'].append(l_dataset)\n",
    "                        found = True\n",
    "\t\t\t### Search in GCS\n",
    "                if found == False:\n",
    "                    found = False\n",
    "                    for i in range(0, len(gcs_contents)):\n",
    "                        regex = r'(\\w*{0}\\w*)'.format(l_dataset)\n",
    "                        matches = re.search(regex, str(gcs_contents[i]))\n",
    "                        if matches:\n",
    "                            dic_result['GCS']['Filename'].append(l_dataset)\n",
    "                            found = True\n",
    "\n",
    "                if found == False:\n",
    "                    found = False\n",
    "\t\t\t\t\t### Search in GS\n",
    "                    ID_gS = gdr.find_file_id(file_name=l_dataset)\n",
    "\n",
    "                    if sheetindex < len(sheetnames):\n",
    "                        sheetName = sheetnames[sheetindex]\n",
    "                        dic_result['GS']['sheetname'].append(sheetName)\n",
    "                        dic_result['GS']['Filename'].append(l_dataset)\n",
    "                        dic_result['GS']['ID'].append(ID_gS)\n",
    "                        sheetindex += 1\n",
    "\n",
    "### Add destination\n",
    "\n",
    "    if destination_engine == 'GCP':\n",
    "        nb_cells[2][1][5] = nb_cells[2][1][5].replace('None',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  notebook_name + '.gz')\n",
    "        nb_cells[2][1][6] = nb_cells[2][1][6].replace('None',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  notebook_name)\n",
    "    else:\n",
    "        nb_cells[2][1][4] = nb_cells[2][1][4].replace('None',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  notebook_name)\n",
    "\n",
    "### Add load data\n",
    "    dic_load_data = {\n",
    "\t'BQ':{'markdown': [],\n",
    "\t\t\t'code':[]\n",
    "\t\t   },\n",
    "\t'GCS': {'markdown': [],\n",
    "\t\t\t'code':[]\n",
    "\t\t   },\n",
    "\t'GS':{'markdown': [],\n",
    "\t\t\t'code':[]\n",
    "\t\t   }\n",
    "}\n",
    "    template_gcs_load = \"\"\"\n",
    "gcp.download_blob(bucket_name = '{0}',\n",
    "\t\t\t\t  destination_blob_name = '{1}',\n",
    "\t\t\t\t  source_file_name = '{2}')\n",
    "\n",
    "    df_{3} = pd.read_csv('{4}',\n",
    "\t\t\t\t\t  compression='gzip',\n",
    "\t\t\t\t\t  header=0,\n",
    "\t\t\t\t\t  sep=',',\n",
    "\t\t\t\t\t  quotechar='\"',\n",
    "\t\t\t\t\t  error_bad_lines=False\n",
    "\t\t\t\t\t  )\n",
    "    \"\"\"\n",
    "\n",
    "    template_gcs_md = \"\"\"\n",
    "## Load {0} from Google Cloud Storage\n",
    "\n",
    "    Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "#### GCS\n",
    "##### Only one file from  GCS?\n",
    "\n",
    "    if dic_result['GCS']['Filename']:\n",
    "        for f, file in enumerate(dic_result['GCS']['Filename']):\n",
    "            filename = dic_result['GCS']['Filename'][f]\n",
    "            for p, bucket in enumerate(gcs_contents):\n",
    "                for i, blob in enumerate(bucket['blob']):\n",
    "                    regex = r'(\\w*{0}\\w*)'.format(file)\n",
    "                    matches = re.search(regex, str(blob))\n",
    "\t\t\t\t\n",
    "                    if matches:\n",
    "                        df_ = file.split(\".\")[0]\n",
    "\t\t\t\t\t#path = gcs_contents[p]['blob'][3].replace(\n",
    "\t\t\t\t\t#'/' + file, '')\n",
    "\t\t\t\t\t### get path\n",
    "                        regex = r\"/[^/]*$\"\n",
    "                        path = re.sub(regex, \"\", blob) \n",
    "                        bucket_name  = bucket['Bucket']\n",
    "                        ggs_cell = template_gcs_load.format(bucket_name,\n",
    "\t\t\t\t\t\t\t\t\t\t path,\n",
    "\t\t\t\t\t\t\t\t\t\t filename,\n",
    "\t\t\t\t\t\t\t\t\t\t df_,\n",
    "\t\t\t\t\t\t\t\t\t\t filename\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   )\n",
    "                        ggs_md = template_gcs_md.format(filename)\n",
    "                        dic_load_data['GCS']['markdown'].append(ggs_md)\n",
    "                        dic_load_data['GCS']['code'].append(ggs_cell)\n",
    "\n",
    "        dic_destination_inf = {\n",
    "            'Bucket':bucket_name,\n",
    "            'destination_blob_name':path,\n",
    "            'new_destination_blob': path.replace('Raw_', 'Processed_')\n",
    "\t}\n",
    "    else:\n",
    "\t#### We don't know where to save the dataset if no table uploaded from GCS\n",
    "        dic_destination_inf = {\n",
    "            'Bucket':'NEED TO DEFINE',\n",
    "            'destination_blob_name':'NEED TO DEFINE',\n",
    "            'new_destination_blob':  'XXXXX/Processed_'\n",
    "\t}\n",
    "##### GBQ\n",
    "    template_gbq_load = \"\"\"\n",
    "      query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM {0}.{1} \"\n",
    "\n",
    "        )\n",
    "\n",
    "    df_{1} = gcp.upload_data_from_bigquery(query = query, location = 'US')\n",
    "    df_{1}.head()\n",
    "    \"\"\"\n",
    "\n",
    "    template_gbq_md = \"\"\"\n",
    "    ## Load {0} from Google Big Query\n",
    "\n",
    "    Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    \"\"\"\n",
    "    if dic_result['BQ']['Filename']:\n",
    "        for f, file in enumerate(dic_result['BQ']['Filename']):\n",
    "            filename = dic_result['BQ']['Filename'][f]\n",
    "#### Note need to search for thr bigquery dataset. Right now, use the predefined one in the argument\n",
    "            cell_gbq = template_gbq_load.format(bigquery_dataset,\n",
    "                filename)\n",
    "            cell_md = template_gbq_md.format(filename)\n",
    "            dic_load_data['BQ']['markdown'].append(cell_md)\n",
    "            dic_load_data['BQ']['code'].append(cell_gbq)\n",
    "\n",
    "##### GS\n",
    "    template_gs_load = \"\"\"\n",
    "    ### Please go here {0}\n",
    "    ### To change the range\n",
    "\n",
    "    sheetid = '{1}'\n",
    "    sheetname = '{2}'\n",
    "\n",
    "    df_{3} = gdr.upload_data_from_spreadsheet(sheetID = sheetid,\n",
    "    sheetName = sheetname,\n",
    "         to_dataframe = True)\n",
    "    df_{3}.head()\n",
    "    \"\"\"\n",
    "\n",
    "    template_gs_md = \"\"\"\n",
    "    ## Load {0} from Google Spreadsheet\n",
    "\n",
    "    Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    Profiling will be available soon for this dataset\n",
    "\n",
    "    \"\"\"\n",
    "    if dic_result['GS']['ID']:\n",
    "        for f, iD in enumerate(dic_result['GS']['ID']):\n",
    "            sheetid = iD\n",
    "            sheetname = dic_result['GS']['sheetname'][f]\n",
    "            filename = dic_result['GS']['Filename'][f]\n",
    "            url_gs = 'https://docs.google.com/spreadsheets/d/{0}'\n",
    "            url_gs = url_gs.format(sheetid)\n",
    "            cell_gs = template_gs_load.format(url_gs,\n",
    "\t\t\t\t\t\t\t\t\t  sheetid,\n",
    "\t\t\t\t\t\t\t\t\t  sheetname,\n",
    "\t\t\t\t\t\t\t\t\t  filename)\n",
    "\n",
    "            cell_md = template_gs_md.format(filename)\n",
    "            dic_load_data['GS']['markdown'].append(cell_md)\n",
    "            dic_load_data['GS']['code'].append(cell_gs)\n",
    "\n",
    "##### Special treatment for prefix\n",
    "#### 1 find path in GCS with dot. It means the element in\n",
    "#### gcs_contents is a filename\n",
    "#### 2 get the path of the blob, ie remove filename from path\n",
    "#### 3 Drop duplicate path\n",
    "\n",
    "    if dataset_suffix:\n",
    "        regex_dot = r\"[.]\"\n",
    "        regex_slash = r\".*(?=/.)\"\n",
    "        list_blob_dot = []\n",
    "        for bucket in gcs_contents:\n",
    "            for blob in bucket['blob']:\n",
    "                matches = re.search(regex_dot, blob)\n",
    "                if matches:\n",
    "                    matches_ = re.search(regex_slash, blob)\n",
    "                    full_path = '{0}/{1}'\n",
    "                    full_path = full_path.format(bucket['Bucket'], matches_.group())\n",
    "                    list_blob_dot.append(full_path)\n",
    "        list_blob_dot = list(dict.fromkeys(list_blob_dot))\n",
    "\n",
    "#### We now have a list of paths. We will check which one has the preffix\n",
    "        l_dataset = []\n",
    "        for blob in list_blob_dot:\n",
    "            split_ = blob.split(\"/\",  1)\n",
    "            bucket = split_[0]\n",
    "            blob_ = split_[1]\n",
    "            prefix = '{0}/{1}'.format(blob_, dataset_suffix)\n",
    "            l_blob =  gcp.list_blob(bucket = bucket, prefix = prefix)\n",
    "            if l_blob['blob']:\n",
    "                break\n",
    "        for d in l_blob['blob']:\n",
    "            split_ = d.rsplit(\"/\",  1)\n",
    "            l_dataset.append(split_[1])   \n",
    "\n",
    "        template_prefix = \"\"\"\n",
    "    df_load_data = pd.DataFrame()\n",
    "    for dataset in {0}:\n",
    "    gcp.download_blob(bucket_name = '{1}',\n",
    "                  destination_blob_name = '{2}',\n",
    "                  source_file_name = dataset)\n",
    "                  \n",
    "    df_temp = pd.read_csv(dataset,\n",
    "                          compression='gzip',\n",
    "                          header=0,\n",
    "                          sep=',',\n",
    "                          quotechar='\"',\n",
    "                          error_bad_lines=False)\n",
    "    df_load_data = df_load_data.append(df_temp)\n",
    "    df_load_data.head()\n",
    "    \"\"\"\n",
    "\n",
    "#### Append to the dic\n",
    "\n",
    "        ggs_cell = template_prefix.format(l_dataset,bucket, blob_)\n",
    "        ggs_md = template_gcs_md.format(notebook_name)\n",
    "        dic_load_data['GCS']['markdown'].append(ggs_md)\n",
    "        dic_load_data['GCS']['code'].append(ggs_cell)\n",
    "\n",
    "\t### If preffix, then override the final destination for GCP\n",
    "\t### If list dataset has both preffix and not prefix for GCP\n",
    "\t### origin, then preffix overide the destination cell\n",
    "\n",
    "        dic_destination_inf = {\n",
    "\t\t'Bucket':bucket,\n",
    "\t\t'destination_blob_name':blob_,\n",
    "\t\t'new_destination_blob': blob_.replace('Raw_', 'Processed_')\n",
    "\t}\n",
    "\n",
    "### Add markdown origin\n",
    "    template_ds = '\\n - {0}'\n",
    "    l_gcs = []\n",
    "    gs_url = '[{0}](https://docs.google.com/spreadsheets/d/{1})'\n",
    "    for key, value in dic_result.items():\n",
    "        if key == 'BQ':\n",
    "            l_gcs.append('\\n ### Big Query Dataset \\n')\n",
    "            for file in value['Filename']:\n",
    "                cell_gcs = template_ds.format(file)\n",
    "                l_gcs.append(cell_gcs)\n",
    "        elif key == 'GCS':\n",
    "            l_gcs.append('\\n ### Google Cloud Storage Dataset \\n')\n",
    "            for file in value['Filename']:\n",
    "                cell_gcs = template_ds.format(file)\n",
    "                l_gcs.append(cell_gcs)\n",
    "            if dataset_suffix:\n",
    "                cell_gcs = template_ds.format(dataset_suffix)\n",
    "                l_gcs.append(cell_gcs)\n",
    "        else:\n",
    "            l_gcs.append('\\n ### Google Spreadsheet Dataset \\n')\n",
    "            for i, file in enumerate(value['Filename']):\n",
    "                cell_gs = template_ds.format(file)\n",
    "                sheet_name = value['Filename'][i]\n",
    "                sheet_ID = value['ID'][i]\n",
    "                url = gs_url.format(sheet_name, sheet_ID)\n",
    "                cell_gs = template_ds.format(url)\n",
    "                l_gcs.append(cell_gs)\n",
    "    cell_datasource = ' '.join(l_gcs)\n",
    "\n",
    "    nb_cells[1][1] = cell_datasource\n",
    "\n",
    "### Change profiling\n",
    "    nb_cells[9][1][2] = nb_cells[9][1][2].replace(\"NAME\", notebook_name)\n",
    "\n",
    "### Upload to the Cloud\n",
    "    template_upload_bqgcs_md = \"\"\"\n",
    "\n",
    "### Move to GCS and BigQuery\n",
    "\n",
    "We move the dataset to the following:\n",
    "\n",
    "- **bucket**: *{0}*\n",
    "\n",
    "- **Destination_blob**: *{1}*\n",
    "- **name**:  *{2}.gz*\n",
    "- **Dataset**: *{3}*\n",
    "\n",
    "- **table**: *{2}*\n",
    "\n",
    "### GCS\n",
    "\n",
    "We first need to save *{2}* with `.gz` extension locally then we can move it\n",
    "to GCS\n",
    "\"\"\"\n",
    "\n",
    "    template_upload_bqgcs = \"\"\"\n",
    "### First save locally\n",
    "df_final.to_csv(\n",
    "\t'{0}.gz',\n",
    "\tsep=',',\n",
    "\theader=True,\n",
    "\tindex=False,\n",
    "\tchunksize=100000,\n",
    "\tcompression='gzip',\n",
    "\tencoding='utf-8')\n",
    "\n",
    "### Then upload to GCS\n",
    "bucket_name = '{1}'\n",
    "destination_blob_name = '{2}'\n",
    "source_file_name = '{0}.gz'\n",
    "gcp.upload_blob(bucket_name, destination_blob_name, source_file_name)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    template_upload_bq = \"\"\"\n",
    "\n",
    "### Move to bigquery\n",
    "bucket_gcs ='{0}/{1}/{2}.gz'\n",
    "gcp.move_to_bq_autodetect(dataset_name= '{3}',\n",
    "\t\t\t\t\t\t\t name_table= '{2}',\n",
    "\t\t\t\t\t\t\t bucket_gcs=bucket_gcs)\n",
    "\"\"\"\n",
    "\n",
    "    if destination_engine == 'GCP':\n",
    "        mkdown_output = template_upload_bqgcs_md.format(dic_destination_inf['Bucket'],\n",
    "\t\t\t\t\t\t\t dic_destination_inf['new_destination_blob'],\n",
    "\t\t\t\t\t\t\t notebook_name,\n",
    "\t\t\t\t\t\t\t bigquery_dataset\n",
    "\t\t\t\t\t\t\t)\n",
    "        code_output_gcs = template_upload_bqgcs.format(notebook_name,\n",
    "\t\t\t\t\t\t\t dic_destination_inf['Bucket'],\n",
    "\t\t\t\t\t\t\t dic_destination_inf['new_destination_blob']\n",
    "\t\t\t\t\t\t\t)\n",
    "        code_output_bq = template_upload_bq.format(dic_destination_inf['Bucket'],\n",
    "\t\t\t\t\t\t  dic_destination_inf['new_destination_blob'],\n",
    "\t\t\t\t\t\t  notebook_name,\n",
    "\t\t\t\t\t\t  bigquery_dataset\n",
    "\t\t\t\t\t\t )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#### Generate nb\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cell_left = 'cell_{0}'\n",
    "    cell_right = '{0}'\n",
    "\n",
    "\n",
    "    dic_cells={}\n",
    "    for i, cell in enumerate(nb_cells[0:6]):\n",
    "        _source = ''.join(cell[1])\n",
    "        rhs = cell_right.format(_source)\n",
    "\t#lhs = cell_left.format(i)\n",
    "        if cell[0] == 'markdown':\n",
    "            dic_cells[\"markdown_{}\".format(i)] = rhs\n",
    "        elif cell[0] == 'code':\n",
    "            dic_cells[\"code_{}\".format(i)] = rhs\n",
    "\n",
    "### GCS\n",
    "    next_cells_available = len(dic_load_data['GCS']['markdown']) + 6\n",
    "    for i, markdown in enumerate(dic_load_data['GCS']['markdown']):\n",
    "        _md = ''.join(markdown)\n",
    "        code = dic_load_data['GCS']['code'][i]\n",
    "        _code = ''.join(code)\n",
    "\n",
    "        rhs_md = cell_right.format(_md)\n",
    "        rhs_code = cell_right.format(_code)\n",
    "\t#lhs = cell_left.format(i + 3)\n",
    "        dic_cells[\"markdown_{}\".format(i+5)] = rhs_md\n",
    "        dic_cells[\"code_{}\".format(i + 6)] = rhs_code\n",
    "\n",
    "        next_cells_available = next_cells_available + i\n",
    "\n",
    "### GBQ\n",
    "    for i, markdown in enumerate(dic_load_data['BQ']['markdown']):\n",
    "        _md = ''.join(markdown)\n",
    "        code = dic_load_data['BQ']['code'][i]\n",
    "        _code = ''.join(code)\n",
    "\n",
    "        rhs_md = cell_right.format(_md)\n",
    "        rhs_code = cell_right.format(_code)\n",
    "\t#lhs = cell_left.format(i + 3)\n",
    "        dic_cells[\"markdown_{}\".format(i+next_cells_available)] = rhs_md\n",
    "        dic_cells[\"code_{}\".format(i + next_cells_available + 1)] = rhs_code\n",
    "\n",
    "        next_cells_available = next_cells_available + 1\n",
    "    next_cells_available = next_cells_available + 3\n",
    "### GS\n",
    "    for i, markdown in enumerate(dic_load_data['GS']['markdown']):\n",
    "        _md = ''.join(markdown)\n",
    "        code = dic_load_data['GS']['code'][i]\n",
    "        _code = ''.join(code)\n",
    "\n",
    "        rhs_md = cell_right.format(_md)\n",
    "        rhs_code = cell_right.format(_code)\n",
    "\t#lhs = cell_left.format(i + 3)\n",
    "        dic_cells[\"markdown_{}\".format(i + next_cells_available)] = rhs_md\n",
    "        dic_cells[\"code_{}\".format(i + next_cells_available + 1)] = rhs_code\n",
    "\n",
    "        next_cells_available = next_cells_available + 1\n",
    "    next_cells_available = next_cells_available + 5\n",
    "\n",
    "### Add step cells\n",
    "    _source = ''.join(nb_cells[6][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = rhs\n",
    "    dic_cells[\"code_{}\".format(next_cells_available)] = []\n",
    "    next_cells_available = next_cells_available + 2\n",
    "\n",
    "### profiling\n",
    "\n",
    "    _source = ''.join(nb_cells[8][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = rhs\n",
    "    next_cells_available = next_cells_available + 1\n",
    "\n",
    "    _source = ''.join(nb_cells[9][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"code_{}\".format(next_cells_available)] = rhs\n",
    "    next_cells_available = next_cells_available + 1\n",
    "\n",
    "### upload to cloud\n",
    "    _source = ''.join(nb_cells[10][1])\n",
    "    rhs = cell_right.format(_source)\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = rhs\n",
    "    next_cells_available = next_cells_available + 1\n",
    "\n",
    "    dic_cells[\"markdown_{}\".format(next_cells_available)] = mkdown_output\n",
    "    dic_cells[\"code_{}\".format(next_cells_available + 1)] = code_output_gcs\n",
    "    dic_cells[\"code_{}\".format(next_cells_available + 2)] = code_output_bq\n",
    "\n",
    "### generate nb\n",
    "    nb['cells'] = []\n",
    "    regex = r\"^markdown\"\n",
    "    for k, v in dic_cells.items():\n",
    "        if re.search(regex, k):\n",
    "            nb['cells'].extend([nbf.v4.new_markdown_cell(v)])\n",
    "        else:\n",
    "            nb['cells'].extend([nbf.v4.new_code_cell(v)])\n",
    "    name_nb = notebook_name + '_preprocessing.ipynb'\n",
    "\n",
    "\n",
    "    os.chdir(path_)\n",
    "    nbf.write(nb, name_nb)\n",
    "    \n",
    "    print(\"Notebook script {} generated sucessfully.\".format(notebook_name))\n",
    "    project = ''\n",
    "    template_bq_input =\"https://console.cloud.google.com/bigquery?project={0}&p={0}&d={1}&t={2}&page=table\"\n",
    "    bq_input = template_bq_input.format(project, bigquery_dataset, notebook_name)\n",
    "    template_gsutil = \"gs//{0}/{1}/{2}.gz\"\n",
    "    if dataset_suffix:\n",
    "        template_gsutil = template_gsutil.format(dic_destination_inf['Bucket'], \n",
    "        dic_destination_inf['destination_blob_name'], dataset_suffix)\n",
    "    else:\n",
    "        template_gsutil = template_gsutil.format(dic_destination_inf['Bucket'], \n",
    "        dic_destination_inf['destination_blob_name'], 'UNKNOWN')\n",
    "\n",
    "    studio_ =\"auto-nb-studio -u '{0}' -c 'GCP' -n '{1}' -p '{2}' -d 'ADD DATE VAR' -m 'US' -t '/PATH CREDENTIAL'\"\n",
    "    studio_ = studio_.format(username, notebook_name, bigquery_dataset)\n",
    "\n",
    "    print( \"\"\"\n",
    "Please, go to the section Inputs_datasource from\n",
    "    https://coda.io/d/MasterFile-Database_dvfMWDBnHh8/Main_suj76#_lubx_ \\n\n",
    " paste the following information \\n \n",
    " Storage:\n",
    " - Storage: GCS\n",
    " - Top_level: {0}\n",
    " - Path: {1}\n",
    " - Filename: {2}.gz\n",
    " - Description: Add detailed description\n",
    " - Dataset_documentation: If any, add URL\n",
    " - Size: Go to console to get the size\n",
    " - Status: Closed\n",
    " - Source_data:{3}\n",
    " - Link_methodology: If any, add URL\n",
    " - JupyterStudio: Leave unselect\n",
    " - Profiling: Leave unselect \\n\n",
    " Big Query :\n",
    "  - Storage: BQ\n",
    "  - Top_level: {5}\n",
    "  - Path: None\n",
    "  - Filename: {2}\n",
    "  - Source_data:{4}\n",
    "  - JupyterStudio: Use \\n \n",
    " {6} \\n\n",
    "  to create a notebook for the studio, then select the checkbox. Only if Dataset in BigQuery!\n",
    " - Profiling: select the checkbox\n",
    "      \"\"\".format(dic_destination_inf['Bucket'], dic_destination_inf['new_destination_blob'],\n",
    "     notebook_name, template_gsutil, bq_input, bigquery_dataset, studio_)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "butt_act = widgets.Button(description='Add table')\n",
    "outt_act = widgets.Output()\n",
    "\n",
    "\n",
    "def on_butt_clicked_act(_):\n",
    "    #outt.clear_output()\n",
    "    \n",
    "    dic_ ={\n",
    "    'notebook_name' : text.value,\n",
    "    'project_name' : dropdown_prject.value,\n",
    "    'input_datasets' : ast.literal_eval(spread_text.value),\n",
    "    'sheetnames' : ast.literal_eval(sheet_text.value),\n",
    "    'bigquery_dataset' : ast.literal_eval(dataset_text.value),\n",
    "    'destination_engine' : 'GCP',\n",
    "    'name_nb' :'{}_preprocessing.ipynb'.format(text.value),\n",
    "    'path':os.path.join(path.value,dropdown_prject.value, 'Data_preprocessing'),\n",
    "    'path_notebook' : os.path.join(path.value,dropdown_prject.value,\n",
    "                                   'Data_preprocessing',\n",
    "                                   'Template_preprocessing.ipynb')\n",
    "        \n",
    "    }\n",
    "    #print('Begin to create Notebook {}'.format(dic_['notebook_name']))\n",
    "    #generate_notebook(dic_)\n",
    "    if menu.value == \"Connect_source\":\n",
    "        print('Connect_source')\n",
    "    elif menu.value == \"Analytics\":\n",
    "        print(\"Analytics\")\n",
    "    else:\n",
    "        print(\"Studio\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Connect DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2f60af6d164ab6aee7654e369b3860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(Dropdown(description='Select:', options=('Connect_source', 'Analytics', 'Studio')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect_source\n",
      "Analytics\n",
      "Studio\n"
     ]
    }
   ],
   "source": [
    "butt_act.on_click(on_butt_clicked_act)\n",
    "box = widgets.VBox([menu,text, path, dropdown_prject, creds,\n",
    "                    spread_text, sheet_text,\n",
    "                    dataset_text, table_text, butt_act,outt_act])\n",
    "butt.on_click(on_butt_clicked)\n",
    "# display\n",
    "data_spread = widgets.VBox([spreadW, sheetW,butt,outt])\n",
    "butt.on_click(on_butt_clicked1)\n",
    "# display\n",
    "data_table = widgets.VBox([datasetsdW, tablesW,butt1,outt1])\n",
    "children = [box, data_spread, data_table]\n",
    "# initializing a tab\n",
    "tab = widgets.Tab()\n",
    "# setting the tab windows \n",
    "tab.children = children\n",
    "# changing the title of the first and second window\n",
    "tab.set_title(0, 'Parameters')\n",
    "tab.set_title(1, 'Select Spreadsheets')\n",
    "tab.set_title(2, 'Select Tables')\n",
    "tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
